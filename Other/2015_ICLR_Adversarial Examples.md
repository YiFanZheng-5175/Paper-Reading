# Explaining and Harnessing Adversarial Examples

>领域：Adversarial Examples  
>发表在：ICLR  
>文章链接：[https://arxiv.org/abs/1412.6572](https://arxiv.org/abs/1412.6572)

## Introduction

Szegedy 等人 (2014b) 发现了一个有趣的现象：一些机器学习模型，包括最先进的神经网络，容易受到对抗样本的影响。也就是说，这些机器学习模型会错误地分类那些与从数据分布中抽取的正确分类样本略有不同的样本。在许多情况下，各种不同架构的模型，在训练数据的不同子集上训练，会对同一个对抗样本进行错误分类。这表明对抗样本暴露了我们训练算法中的基本盲点。

这些对抗样本的成因一直是个谜，推测性的解释认为这是由于深度神经网络的极端非线性造成的，也许还与模型平均不足和纯粹监督学习问题的正则化不足有关。我们证明了这些推测性假设是不必要的。高维空间中的线性行为足以导致对抗样本。这种观点使我们能够设计一种快速生成对抗样本的方法，从而使对抗训练变得实用。我们表明，对抗训练可以提供额外的正则化益处，超出仅使用 dropout（Srivastava 等人，2014）提供的益处。诸如 dropout、预训练和模型平均等通用正则化策略并不能显著降低模型对对抗样本的脆弱性，但转向非线性模型族，例如 RBF 网络，可以做到这一点。

我们的解释表明，在设计易于训练的线性模型和设计使用非线性效应来抵抗对抗性扰动的模型之间存在着根本的矛盾。从长远来看，通过设计更强大的优化方法，可以成功地训练更非线性的模型，从而有可能摆脱这种权衡。

基于现代机器学习技术的分类器，即使是在测试集上取得了优异的性能，也并没有学习到决定正确输出标签的真正底层概念。相反，这些算法构建了一个“波将金村”，它在自然发生的数据上运行良好，但在访问数据分布中概率不高的空间点时，就会暴露为虚假。这尤其令人失望，因为计算机视觉中的一种流行方法是使用卷积网络特征作为空间，其中欧氏距离近似于感知距离。如果在网络表示中，感知距离极小的图像对应于完全不同的类别，那么这种相似性显然是有缺陷的。

这些结果通常被解释为深度网络的缺陷，尽管线性分类器也存在同样的问题。我们认为了解这种缺陷是一个修复它的机会。事实上，Gu & Rigazio (2014) 和 Chalupka 等人 (2014) 已经开始朝着设计对抗扰动模型迈出第一步，尽管还没有模型能够在保持干净输入的最新准确率的同时成功做到这一点。

## 对抗样本的线性解释

在许多问题中，单个输入特征的精度是有限的。例如，数字图像通常每个像素只使用8位，因此它们会丢弃动态范围内低于1/255的所有信息。由于特征的精度有限，如果扰动η的每个元素都小于特征的精度，则分类器对输入x与对抗性输入 $\tilde{x}$ = x + η做出不同的响应是不合理的。正式地，对于类之间分离良好的问题，我们期望分类器将相同的类分配给 x 和 $\tilde{x}$ ,只要 ||η||∞ < ϵ，其中 ϵ 足够小，以至于可以被与我们的问题相关的传感器或数据存储设备丢弃。

考虑权重向量$\boldsymbol{w}$与对抗样本$\tilde{\boldsymbol{x}}$之间的点积：
$$
\boldsymbol{w}^{\top}\tilde{\boldsymbol{x}} = \boldsymbol{w}^{\top}\boldsymbol{x} + \boldsymbol{w}^{\top}\boldsymbol{\eta}
$$
对抗扰动使得激活值增加$\boldsymbol{w}^{\top}\boldsymbol{\eta}$。在$\boldsymbol{\eta}$的最大范数约束下，我们可以通过令$\boldsymbol{\eta} = \text{sign}(\boldsymbol{w})$来最大化这种增加。如果$\boldsymbol{w}$是$n$维的，且权重向量元素的平均幅度为$m$，那么激活值将增加$emn$。由于$\|\boldsymbol{\eta}\|_{\infty}$不会随着问题维度的增加而增长，但是由$\boldsymbol{\eta}$扰动引起的激活值变化会随着$n$线性增长，所以对于高维问题，我们可以对输入进行许多微小的改变，这些改变累积起来会对输出产生一个较大的变化。我们可以将其视为一种“意外隐写术”，在线性模型中，即使存在多个信号，且其他信号的幅度大得多，该模型也会被迫只关注与它的权重最匹配的信号。

这个解释表明，如果一个简单的线性模型的输入具有足够的维度，那么它也可能存在对抗样本。之前对于对抗样本的解释往往借助于神经网络的一些假设性质，比如它们所谓的高度非线性。我们基于线性的假设更简单，并且也能够解释为什么Softmax回归对对抗样本是脆弱的。

## 非线性模型的线性扰动

对抗样本的线性观点提出了一种快速生成它们的方法。我们假设神经网络过于线性，以至于无法抵御线性对抗扰动。LSTM（Hochreiter & Schmidhuber，1997）、ReLU（Jarrett等人，2009；Glorot等人，2011）以及maxout网络（Goodfellow等人，2013c）都是有意设计成以非常线性的方式运作，这样它们更容易优化。出于同样的原因，更非线性的模型（如sigmoid网络）会经过精心调整，使其大部分时间处于非饱和的、更线性的状态。这种线性行为表明，对线性模型进行低成本的解析扰动也应该会损害神经网络。  

![20250320165137](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250320165137.png)
> 图1：将快速对抗样本生成方法应用于ImageNet上GoogLeNet（Szegedy等人，2014a）的演示。通过添加一个小到难以察觉的向量（其元素等于代价函数关于输入的梯度的元素的符号），我们可以改变GoogLeNet对图像的分类。这里我们的$\epsilon = .007$，对应GoogLeNet将8位图像编码转换为实数后最小位的幅度。  

设$\theta$为模型的参数，$\boldsymbol{x}$为模型的输入，$y$为与$\boldsymbol{x}$相关联的目标（对于有目标的机器学习任务），$J(\theta, \boldsymbol{x}, y)$为训练神经网络所用的代价函数。我们可以围绕$\theta$的当前值对代价函数进行线性化，得到受最大范数约束的最优扰动：  
$$
\boldsymbol{\eta} = \epsilon \text{sign}(\nabla_{\boldsymbol{x}} J(\theta, \boldsymbol{x}, y))
$$  
我们将此称为生成对抗样本的“快速梯度符号法”。注意，所需的梯度可通过反向传播高效计算。  

我们发现，该方法能可靠地使各种模型对其输入错误分类。ImageNet上的演示见图1。我们发现，使用$\epsilon = .25$时，在MNIST（？）测试集上，一个浅层softmax分类器的错误率达到99.9%，对我们生成的对抗样本的平均置信度为79.3%。同样，使用$\epsilon = .1$时，在经过预处理的CIFAR-10（Krizhevsky & Hinton，2009）测试集上使用卷积maxout网络，我们得到87.15%的错误率，分配给错误标签的平均概率为96.6%。其他生成对抗样本的简单方法也是可行的。例如，我们还发现，将$\boldsymbol{x}$沿梯度方向旋转一个小角度也能可靠地生成对抗样本。  

这些简单、低成本的算法能够生成被错误分类的样本，这一事实为我们将对抗样本解释为线性性质的结果提供了证据。这些算法作为加速对抗训练，甚至只是分析已训练网络的一种方式，也是有用的。

## 线性模型的对抗训练与权重衰减

我们能考虑的最简单模型或许是逻辑回归。在此情形下，快速梯度符号法（fast gradient sign method）是精确的。我们可借助该案例，对对抗样本在简单场景中的生成方式形成直观理解。具指导意义的图像见图 2。  

![20250320171528](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250320171528.png)  
> 图2：快速梯度符号法应用于逻辑回归（在此场景中，该方法并非近似，而是最大范数框中真正最具破坏性的对抗样本）。a）在MNIST数据集上训练的逻辑回归模型的权重；b）MNIST数据集上训练的逻辑回归模型权重的符号，此为最优扰动。尽管模型容量低且拟合效果好，这种扰动对人类观察者而言，难以识别出与数字3和7之间的关系有任何关联；c）MNIST数据集中的数字3和7，在这些样本的“3对7”判别任务上，逻辑回归模型错误率为1.6%；d）当$\epsilon=0.25$时，逻辑回归模型的快速梯度符号对抗样本，此时模型在这些样本上的错误率达99%。

若训练单个模型识别标签 $y \in \{-1, 1\}$，其中 $P(y=1) = \sigma(\boldsymbol{w}^\top \boldsymbol{x} + b)$，$\sigma(z)$ 为逻辑 sigmoid 函数，那么训练过程基于对下式的梯度下降：  
$$
\mathbb{E}_{\boldsymbol{x}, y \sim p_{\text{data}}} \zeta\left(-y(\boldsymbol{w}^\top \boldsymbol{x} + b)\right)
$$  
其中 $\zeta(z) = \log(1 + \exp(z))$ 是 softplus 函数。基于梯度符号扰动，我们可推导出针对 $\boldsymbol{x}$ 的最坏情况对抗扰动（而非 $\boldsymbol{x}$ 本身）的简单解析训练形式。  

需注意，梯度的符号恰为 $-\text{sign}(\boldsymbol{w})$，且 $\boldsymbol{w}^\top \text{sign}(\boldsymbol{w}) = \|\boldsymbol{w}\|_1$。因此，逻辑回归的对抗版本旨在最小化：  
$$
\mathbb{E}_{\boldsymbol{x}, y \sim p_{\text{data}}} \zeta\left(y(\epsilon\|\boldsymbol{w}\|_1 - \boldsymbol{w}^\top \boldsymbol{x} - b)\right).  
$$  
这与 $L^1$ 正则化有一定相似性，但也存在重要差异。最显著的是，$L^1$ 惩罚项在训练中是从模型激活值中减去，而非加入训练成本。这意味着，若模型学习到足够置信的预测（使 $\zeta$ 饱和），惩罚项最终可能消失。但这并非必然——在欠拟合状态下，对抗训练只会加剧欠拟合。因此，我们可认为 $L^1$ 权重衰减比对抗训练更“极端”，因其在良好边际情况下仍未失效。  

若从逻辑回归扩展到多分类 softmax 回归，$L^1$ 权重衰减会更保守，因为它将 softmax 的每个输出视为可独立扰动的，而实际上通常无法找到单个 $\eta$ 与所有类别权重向量对齐。在深度网络（含多个隐藏层）中，权重衰减对扰动造成的损害估计更夸张。由于 $L^1$ 权重衰减高估了对抗扰动的损害程度，使用更小的 $L^1$ 权重衰减系数比与特征精度相关的 $\epsilon$ 更必要。在 MNIST 上训练 maxout 网络时，使用 $\epsilon = 0.25$ 的对抗训练取得了良好结果。对第一层应用 $L^1$ 权重衰减时，发现即使系数为 $0.0025$ 也过大，导致模型在训练集上陷入超 5% 的错误率。更小的权重衰减系数则在无正则化收益的情况下实现了成功训练。

## 深度网络的对抗训练

对深度网络易受对抗样本影响的批评存在一定误导性，因为与浅层线性模型不同，深度网络至少能够表示抵抗对抗扰动的函数。通用近似定理（Hornik 等人，1989）表明，只要隐藏层单元足够多，含至少一个隐藏层的神经网络就能以任意精度表示任何函数。而浅层线性模型无法在训练点附近保持恒定，同时又为不同训练点分配不同输出。  

当然，通用近似定理并未阐明训练算法能否发现具备所有期望属性的函数。显然，标准监督训练未要求所选函数抵抗对抗样本，这一特性必须以某种方式融入训练过程。  

我们通过两项调整解决该问题：首先，扩大模型规模，每层使用 1600 个单元（原始 maxout 网络处理此问题时仅用 240 个单元）。无对抗训练时，模型轻微过拟合，测试集错误率为 1.14%；有对抗训练时，验证集错误率随时间趋于稳定，且基于 L-BFGS 生成的昂贵对抗样本增加了广泛实验的难度。Szegedy 等人（2014b）指出，通过混合对抗样本与干净样本训练，神经网络可实现一定程度的正则化。对抗样本训练与其他数据增强不同：常规增强采用测试集可能出现的变换（如平移），而此方法使用自然场景中罕见但能暴露模型缺陷的输入。实验中我们取 $\alpha=0.5$，其他值或许效果更优，但初步设定已满足需求，无需进一步探索。通过原始模型生成的对抗样本，在对抗训练模型上错误率为 19.0%；通过新模型生成的对抗样本，在原始模型上错误率达 40.9%。当对抗训练模型出现误分类时，零均值与零协方差的噪声难以有效抵御对抗样本——任意参考向量与该噪声向量的期望点积为零，意味着多数情况下噪声无实质影响，而非生成更难处理的输入。

![20250320173008](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250320173008.png)  
> 图3：MNIST 数据集上训练的 maxout 网络权重可视化，每行展示单个 maxout 单元的滤波器。左：简单训练的模型；右：含对抗训练的模型。  

实际上，噪声常导致更低的目标函数值。对抗训练可视为在含噪输入中挖掘难例，仅关注强抵抗分类的噪声点以提升训练效率。作为对照，我们训练了一个 maxout 网络：对像素随机添加 $\pm\epsilon$，或在 $U(-\epsilon, \epsilon)$ 范围内添加噪声。两种方式在快速梯度符号对抗样本上的错误率分别为 86.2%（置信度 97.3%）和 90.4%（置信度 97.8%）。  

由于符号函数处处为零或无定义，基于快速梯度符号法的对抗目标函数梯度下降，无法让模型预测对抗者对参数变化的反应。若改用基于小旋转或缩放梯度的对抗样本，扰动过程可纳入对抗者反应的学习，但我们未发现此过程的正则化效果——或许因这类对抗样本较易处理。  

一个自然的问题是：扰动输入、隐藏层，还是两者兼具更优？结果并不统一。Szegedy 等人（2014b）发现，对抗扰动作用于隐藏层时正则化效果最佳（基于 sigmoidal 网络）。但在我们的快速梯度符号实验中，隐藏单元激活无界的网络，会通过增大隐藏单元激活值响应扰动，因此通常只需扰动原始输入。在饱和模型（如 Rust 模型）中，扰动输入与隐藏层效果相当。对隐藏层进行旋转扰动，可缓解激活无界增长的问题，使加性扰动更小。我们成功训练了含隐藏层旋转扰动的 maxout 网络，但其正则化效果不及输入层加性扰动。对抗训练的价值仅体现于模型有能力学习抵抗对抗样本，而这依赖通用近似定理的适用。由于神经网络的最后一层（线性 sigmoid 或线性 - softmax 层）并非最终隐藏层函数的通用近似器，对最终隐藏层应用对抗扰动时易出现欠拟合。实验也确实发现了该效应——使用隐藏层扰动训练的最终结果，从未涉及对最终隐藏层的扰动。

## 不同类型的模型容量  

对抗样本的存在看似反直觉，原因之一是我们大多对高维空间缺乏直观认知。我们生活在三维空间，不适应数百维空间中微小影响累积成显著效果。还有另一种情况让直觉失效：许多人认为低容量模型无法做出多样且置信度高的预测，这并不正确。部分低容量模型确实有此表现，例如浅层径向基函数（RBF）网络，其  
$$p(y=1\,|\,\boldsymbol{x}) = \exp\left((\boldsymbol{x}-\mu)^\top \boldsymbol{\beta}(\boldsymbol{x}-\mu)\right)$$  
仅能在$\mu$附近自信预测正类存在，其他区域则默认预测正类不存在，或给出低置信度预测。  

RBF网络对对抗样本有天然免疫，体现在被欺骗时置信度低。无隐藏层的浅层RBF网络，在MNIST数据集上，使用快速梯度符号法（$\epsilon=0.25$）生成的对抗样本测试中，错误率达55.4%，但错误样本的置信度仅1.2%，干净测试样本的平均置信度为60.6%。我们不能期望低容量模型在空间所有点都输出正确答案，但它在不“理解”的样本点上，通过显著降低置信度做出正确响应。  

遗憾的是，RBF单元对任何显著变换都无不变性，泛化能力差。可将线性单元与RBF单元视为精确率-召回率权衡曲线上的不同点：线性单元通过对特定方向的所有输入响应实现高召回率，但在陌生场景中响应过强，可能导致精确率低；RBF单元仅对空间特定点响应以实现高精确率，但牺牲了召回率。受此启发，我们探索包含二次单元的多种模型（如深度RBF网络），发现这是艰巨任务——用随机梯度下降（SGD）训练时，具备足够二次抑制以抵抗对抗扰动的模型，训练集错误率都很高。

## 为何对抗样本具有泛化性？

对抗样本的一个有趣特征是：为某一模型生成的样本，常被其他模型误分类，即便这些模型架构不同，或基于不相交的训练集训练。此外，当不同模型对某对抗样本误分类时，它们对该样本的类别判断往往一致。基于极端非线性和过拟合的解释难以说明这一现象——为何多个非过参数化的非线性模型，会持续以相同方式标注分布外的样本点？从“对抗样本像实数中的有理数那样精细划分空间”这一假设看，该现象尤其令人惊讶，因为此假设认为对抗样本虽常见，却仅出现于极为精确的位置。  

在线性视角下，对抗样本存在于广泛的子空间中。方向 $\eta$ 只需与代价函数的梯度有正点积，且 $\epsilon$ 足够大即可。图 4 佐证了这一现象：通过追踪不同 $\epsilon$ 值，发现对抗样本存在于快速梯度符号法定义的一维子空间的连续区域，而非精细“区域”中。这解释了对抗样本为何大量存在，以及为何被一个分类器误分类的样本，很可能也被另一个分类器误分类。

![20250320174511](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250320174511.png)  
>图4：通过追踪不同的$\epsilon$值，我们可见，只要沿正确方向调整，几乎对任何足够大的$\epsilon$值，对抗样本都会稳定出现。仅在数据中$\boldsymbol{x}$所在的一个“薄流形”上才会出现正确分类。$\mathbb{R}^n$的大部分区域由对抗样本和“无意义类别样本”（见附录）构成。此图基于简单训练的 maxout 网络绘制。  - **左图**：当对单个输入样本改变 $\epsilon$ 时，展示 10 个 MNIST 类别对应 softmax 层输入的示意图（正确类别是 4）。可见，每个类别的未归一化对数概率与 $\epsilon$ 呈明显的分段线性关系，且错误分类在广泛的 $\epsilon$ 值区域内稳定。此外，当 $\epsilon$ 增至进入“无意义输入”范围时，预测变得非常极端。  - **右图**：生成曲线的输入（左上 = 负 $\epsilon$，右下 = 正 $\epsilon$，黄色框表示正确分类的输入）。

为解释多个分类器对对抗样本标注相同类别，我们提出假设：按当前方法训练的神经网络，均类似于在相同训练集上学习的线性分类器。当在训练集的不同子集上训练时，该参考分类器能使不同模型学习到近似的分类权重，这源于机器学习算法的泛化能力。底层分类权重的稳定性，进而导致对抗样本的稳定性。  

为验证该假设，我们在深度 maxout 网络上生成对抗样本，并用浅层 softmax 网络和浅层 RBF 网络进行分类。在被 maxout 网络误分类的样本上，RBF 网络仅 16.0% 能正确预测 maxout 网络的类别标注，而 softmax 分类器 54.6% 能正确预测。这些数据主要受不同模型错误率差异的影响。但若仅关注两模型均犯错的情况，softmax 回归预测 maxout 类别正确率为 84.6%，RBF 网络仅 54.3% 能预测 maxout 的类别。作为对比，RBF 网络预测 softmax 回归类别的正确率为 53.6%，表明其行为具有强线性成分。我们的假设无法解释 maxout 网络的所有错误，或所有线性模型的错误，但显然，其中相当比例的错误与跨模型泛化的主要原因一致。

## 其他假设

我们现在思考并反驳关于对抗样本存在的一些其他假设。首先，一种假设认为生成式训练能为训练过程提供更多约束，或让模型学会区分“真实”与“虚假”数据，且仅对“真实”数据产生置信预测。MP-DBM（Goodfellow 等人，2013a）是测试该假设的理想模型，其推理过程在 MNIST 上实现了良好的分类准确率（错误率 0.88%），且该推理过程可微。其他生成式模型要么具有不可微的推理过程（使对抗样本更难计算），要么需额外的非生成式判别器模型才能在 MNIST 上获得高分类准确率。对 MP-DBM 而言，我们可确定是生成式模型本身在响应对抗样本，而非顶层的非生成式分类器模型。实验发现该模型易受对抗样本影响：当 $\epsilon = 0.25$ 时，对 MNIST 测试集生成的对抗样本，错误率达 97.5%。尽管可能存在其他形式的生成式训练赋予模型抗性，但显然仅“生成式”这一属性本身并不足够。  

另一种关于对抗样本为何存在的假设认为，单个模型有异常特性，但对多个模型取平均可消除对抗样本。为验证该假设，我们在 MNIST 上训练了由 12 个 maxout 网络组成的集成模型，每个网络在初始化权重、生成 dropout 掩码、选择随机梯度下降小批量数据时，使用不同随机数种子。当对抗样本设计为扰动整个集成（$\epsilon = 0.25$）时，集成模型错误率为 91.1%；若对抗样本仅扰动集成中的单个成员，错误率降至 87.9%。可见，模型集成对对抗扰动仅提供有限的抵抗能力。

## 总结与讨论

本文总结得出以下观点：  

- 对抗样本可解释为高维点积的一种属性，其产生是模型“过于线性”而非“过于非线性”的结果。  
- 对抗样本在不同模型间的泛化性，可解释为：对抗扰动与模型权重向量高度对齐，且不同模型在训练执行相同任务时学习到相似函数。  
- 扰动方向最为关键，而非空间中的具体点。空间并非布满像有理数那样精细划分实数的对抗样本“区域”。  
- 因扰动方向最为关键，对抗扰动可在不同干净样本间泛化。  
- 本文引入了一类快速生成对抗样本的方法。  
- 证实对抗训练可实现正则化，甚至比 dropout 更有效。  
- 对照实验表明，简单但低效的正则化方法（如 $L^1$ 权重衰减、添加噪声）无法复现该效果。  
- 易优化的模型易受扰动。  
- 线性模型缺乏抵抗对抗扰动的能力；仅满足通用近似定理的结构（含隐藏层）需经训练以抵抗对抗扰动。  
- RBF 网络对对抗样本有抗性。  
- 以输入分布为建模目标的训练模型无法抵抗对抗样本。  
- 模型集成无法抵抗对抗样本。  

附录中关于“无意义类别样本”的进一步观点：  

- 无意义类别样本普遍存在且易生成。  
- 浅层线性模型无法抵抗无意义类别样本。  
- RBF 网络可抵抗无意义类别样本。  

基于梯度的优化是现代人工智能的核心。使用设计得足够线性的网络（无论是 ReLU 或 maxout 网络、LSTM，还是经精心配置未过度饱和的 sigmoid 网络），我们能解决多数关注训练集的问题。但对抗样本的存在表明：能解释训练数据甚至正确标注测试数据，不代表模型真正理解所执行的任务。实际上，其线性响应在数据分布外的点上过度自信，而这些自信的预测往往高度错误。本文工作表明，通过显式识别问题点并在这些点上修正模型，可部分解决该问题。但也可得出结论：我们使用的模型类别存在内在缺陷。模型易优化的代价是被轻易误导，这推动了优化过程的发展，使模型学习到局部更优的行为。
