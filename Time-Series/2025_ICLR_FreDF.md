# FreDF: Learning to Forecast in Frequency Domain

>领域：时间序列预测  
>发表在：ICLR 2025  
>模型名字： **Fre**quency-enhanced **D**irect **F**orecas  
>文章链接：[FreDF: Learning to Forecast in Frequency Domain](https://arxiv.org/abs/2402.02399)  
>代码仓库：[https://github.com/Master-PLC/FreDF](https://github.com/Master-PLC/FreDF)  
![20250407104951](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407104951.png)

## 一、研究背景与问题提出

### 1.1 研究现状

时间序列建模与经典回归任务的一个关键区别在于自相关的存在，自相关指的是输入序列和标签序列中时间步长之间固有的依赖关系。为了适应输入序列中的自相关，人们开发了多种预测模型，如循环模型（Salinas等人，2020年）、卷积模型（Wu等人，2023年）和图神经网络（Yi等人，2023a）。最近，基于Transformer的模型利用自注意力机制动态评估自相关，受到了广泛关注（Liu等人，2024年；Nie等人，2023年）。同时，将频率分析纳入预测模型的趋势也日益增强。通过在频域中表示输入序列，可以有效地处理输入自相关，这已被证明能提高Transformer（Zhou等人，2022年）和多层感知器（MLP）（Yi等人，2023b）的预测性能。这些开创性工作凸显了自相关和频率分析在先进时间序列建模中的重要性。

另一个关键方面是标签序列内的自相关，其中每个未来时间步都自回归地依赖于其前一步。这种现象被称为标签自相关，是一个值得研究的关键问题。具体而言，目前的预测方法主要采用直接预测（DF）范式（Liu等人，2024年；Nie等人，2023年），该范式通过多输出头同时生成多步预测（Liu等人，2022b），同时优化所有步骤的预测误差。

### 1.2 动机

为了适应自相关，人们开发了各种神经网络架构，这些架构能有效地对输入序列中的自相关进行建模。然而，标签自相关无法通过神经网络架构的修改来处理。为了有效地处理标签自相关，有必要创建专门针对这些依赖性的学习目标。

现代时间序列预测模型主要在多任务学习方式下进行训练，这被称为直接预测（DF）范式。具体来说，DF范式使用一个多输出模型$g_{\theta} : \mathbb{R}^{H \times D} \to \mathbb{R}^{T \times D}$来生成$T$步预测$\hat{Y} = g_{\theta}(L)$ 。模型参数$\theta$通过最小化时间损失来优化：
$$
\mathcal{L}^{(\mathrm{tmp})} := \sum_{t = 1}^{T} \lVert Y_t - \hat{Y}_t \rVert_2^2. \tag{1}
$$

在这个学习目标中，每个预测步的时间损失是独立计算的，将每个未来时间步视为一个单独的任务。虽然这种方法在经验上已证明有效，但它忽略了标签序列$Y$中存在的自相关。具体而言，标签序列是自回归生成的，$Y_{t + 1}$高度依赖于$Y_t$，如图1（a）中的蓝色箭头所示。相比之下，学习目标（1）假设标签序列中的每个步骤都可以独立建模，如图1（a）中的黑色箭头所示。模型假设与数据特征之间的这种不一致给DF范式的学习目标引入了偏差，如定理3.1所示。

![20250407105352](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407105352.png)
**定理3.1（DF的偏差）** 给定输入序列$L$和标签序列$Y$，DF范式的学习目标（1）相对于实际的负对数似然（NLL）存在偏差，表达式为：
$$
\text{Bias} = \sum_{i = 1}^{T} \frac{1}{2 \sigma^2} (Y_i - \hat{Y}_i)^2 - \sum_{i = 1}^{T} \frac{1}{2 \sigma^2 (1 - \rho_i^2)} \left( Y_i - \left( \hat{Y}_i + \sum_{j = 1}^{i - 1} \rho_{ij} (Y_j - \hat{Y}_j) \right) \right)^2, \tag{2}
$$
其中$\hat{Y}_i$表示第$i$步的预测值，$\rho_{ij}$表示在给定$L$的情况下$Y_i$和$Y_j$之间的偏相关，$\rho_i^2 = \sum_{j = 1}^{i - 1} \rho_{ij}^2$。

根据定理3.1，标签自相关$\rho_{ij}$的存在导致损失相对于真实数据的NLL产生偏差。值得注意的是，当标签不相关（$\rho_{ij} = 0$）时，这种偏差会减小到零。因此，标签自相关是训练时间序列预测模型的一个关键因素。

## 二、问题剖析与解决策略

这种方法隐含地假设了标签序列中各时间步之间相互独立，忽略了时间序列预测任务中固有的标签自相关。我们从理论上证明，这种忽视会导致预测偏差，揭示了现有DF范式的一个重大缺陷。

为了解决这个问题，我们引入了频率增强直接预测（FreDF），这是对DF范式的一种简单而有效的改进。其核心思想是在频域中对齐预测值和标签序列，在频域中标签相关性可被有效削弱。这种方法解决了DF范式的适用范围与实际时间序列特征之间的差异，同时保留了DF范式的优点，如样本效率和易于实现。

### 2.1 解决方法

正如定理3.1所表明的，随着标签自相关的减小，学习目标中的偏差也会减小。为了实现这种相关性的降低，一种可行的策略是将标签序列转换为一种能够最小化自相关的表示形式。定义3.2中定义的离散傅里叶变换（DFT）提供了一种直观的方法，它将序列投影到一组正交指数基上。在这个变换后的空间中，标签序列被描述为一组预定义正交时间模式的线性组合，这有效地绕过了时域中的自相关。在定理3.3中，这种变换在减少标签自相关方面的有效性得到了正式化，其中不同频率分量变得不相关。因此，降低的$\rho_{i \neq j}$减小了相对于负对数似然（NLL）的偏差，这有利于时间序列预测模型的训练。

**定义3.2（离散傅里叶变换，DFT）** 序列$Y = [Y_0, \ldots, Y_{T - 1}]$的归一化离散傅里叶变换是将其投影到一组不同频率的正交傅里叶基上。频率$k$的投影计算如下：
$$
F_k = \sum_{t = 0}^{T - 1} Y_t \exp\left( -j \left( \frac{2 \pi k}{T} \right) t \right) / \sqrt{T},
$$
其中$j$是虚数单位，$\exp(\cdot)$是不同$k$值对应的傅里叶基。DFT由投影集合$F = [F_1, \ldots, F_{T - 1}]$组成，表示为$F = \mathcal{F}(Y)$，可以通过复杂度为$O(T \log T)$的快速傅里叶变换（FFT）算法来计算。

**定理3.3（频率分量之间的去相关）** 设$Y$是长度为$T$的零均值、离散时间、广义平稳随机过程。当$T \to \infty$时，DFT系数在不同频率下渐近不相关：
$$
\lim_{T \to \infty} \mathbb{E}[F_k F_{k'}^*] =
\begin{cases}
S_Y(f_k), & \text{如果 } k = k', \\
0, & \text{如果 } k \neq k',
\end{cases}
$$
其中$f_k = \frac{k}{T}$，$S_Y(f)$是$Y$的功率谱密度。

![20250407105352](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407105352.png)

**案例研究**：为了验证我们的理论主张，我们在气象数据集上进行了一个案例研究，如图1所示。实现细节和更多证据见附录A。主要观察结果总结如下：

- **标签自相关的证据**：图1（b）量化了在输入$L$的条件下，标签序列$Y$中不同时间步$Y_i$和$Y_j$之间的偏相关。许多非对角元素具有显著值，大约37.5%超过0.3。这表明在$L$的条件下，$Y$中的不同时间步是相关的，证实了标签自相关的存在。此外，自相关呈现出规则的变化，如图1（b）中交替的亮区和暗区所示，表明序列具有周期性。这种标签自相关使得简单DF范式的学习目标产生偏差，正如定理3.1所证明的。
- **域变换的效果**：图1（c - d）展示了变换后的标签序列$F$不同频率分量之间的偏相关。大多数非对角元素的值可以忽略不计，只有约3.6%超过0.1。这表明将标签序列变换到频域显著降低了不同分量之间的偏相关，证实了定理3.3。标签相关性$\rho_{i \neq j}$的降低导致定理3.1中确定的偏差减小，强调了在频域进行预测以获得更准确和无偏预测的潜力。

![20250407104951](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407104951.png)

如图2所示，输入序列$L$被输入模型以生成$T$步预测，记为$\hat{Y} = g(L)$ 。时间预测误差$\mathcal{L}^{(\mathrm{tmp})}$根据公式（1）计算。随后，预测序列和标签序列都通过快速傅里叶变换（FFT）转换到频域。频域预测误差计算如下：
$$
\mathcal{L}^{(\mathrm{freq})} := \left\lvert \mathcal{F}(\hat{Y}) - \mathcal{F}(Y) \right\rvert_1, \tag{3}
$$
其中$Y \in \mathbb{R}^{T \times D}$，$\lvert \cdot \rvert_1$表示元素级的$\ell_1$范数，即对矩阵内所有元素的绝对值求和。由于FFT是可微的（Wu等人，2021年；Zhou等人，2022年），$\mathcal{L}^{(\mathrm{freq})}$可以使用标准随机梯度下降方法进行优化。鉴于变换后标签序列的数值特性，我们主张在频域使用$\ell_1$损失而非平方损失。具体来说，不同频率分量的幅度往往差异很大；低频分量相比高频分量具有显著更高的幅度，这使得平方损失容易出现不稳定情况。通过使用$\ell_1$损失，我们追求更平衡和稳定的优化过程。

最后，将时间预测误差和频域预测误差融合，权重参数$0 \leq \alpha \leq 1$控制每个误差的相对贡献：
$$
\mathcal{L}^{\alpha} := \alpha \cdot \mathcal{L}^{(\mathrm{freq})} + (1 - \alpha) \cdot \mathcal{L}^{(\mathrm{tmp})}. \tag{4}
$$
通过在频域中对齐预测序列和标签序列，FreDF减轻了标签自相关造成的偏差，同时保留了DFT的优势，包括高效推理和多任务学习能力。此外，FreDF与模型无关，可与各种预测模型$g$（如Transformer和MLP）兼容。这种灵活性显著扩展了FreDF在不同时间序列预测场景中的潜在应用，在这些场景中不同的预测模型可能展现出卓越性能。

## 三、实验验证与结果分析

### 3.1 实验设置

- **数据集**：用于长期预测和插补的数据集包括ETT（4个子集）、ECL、Traffic、Weather和PEMS（Liu等人，2024年）。用于短期预测的数据集是遵循Wu等人（2023年）的M4数据集。每个数据集按时间顺序划分为训练集、验证集和测试集。详细的数据集描述见附录D.1。
- **基线模型**：我们的基线模型包括各种已有的模型，可分为三类：（1）基于Transformer的方法：Transformer（Vaswani等人，2017年）、Autoformer（Wu等人，2021年）、FEDformer（Zhou等人，2022年）、iTransformer（Liu等人，2024年）；（2）基于MLP的方法：DLinear（Zeng等人，2023年）、TiDE（Das等人，2023年）、FreTS（Yi等人，2023b）；（3）其他值得注意的模型：TimesNet（Wu等人，2023年）、MICN（Wang等人，2023b）、TCN（Bai等人，2018年）。
- **实现方式**：基线模型使用Liu等人（2024年）提供的脚本复现。使用Adam优化器（Kingma和Ba，2015年）进行训练，以最小化均方误差（MSE）损失。数据集按时间顺序划分为训练集、验证集和测试集。遵循综合基准测试中概述的协议（Qiu等人，2024年），在测试阶段不使用丢弃最后一个数据点的技巧。当集成FreDF来增强已有模型时，我们遵循公开基准测试中的相关超参数设置（Liu等人，2024年），仅谨慎调整$\alpha$和学习率。实验在英特尔(R) 至强(R) 铂金8383C CPU和英伟达RTX 3090 GPU上进行。更多实现细节见附录D.2。

### 3.2 整体性能

![20250407112741](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407112741.png)

长期预测任务的性能见表1，我们选择iTransformer作为预测模型$g$ ，并使用FreDF对其进行增强。总体而言，FreDF显著提升了iTransformer的性能。例如，在ETTm1数据集上，FreDF将iTransformer的MSE降低了0.019。在其他数据集上也有类似的性能提升，这可归因于FreDF协调了标签自相关与DF范式，验证了FreDF的有效性。

此外，FreDF提升了iTransformer的性能，使其在某些数据集上甚至超越了原本性能优于iTransformer的模型。这表明FreDF带来的改进超出了仅通过专门架构设计所能实现的效果，强调了处理标签自相关和FreDF的重要性。

![20250407112904](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407112904.png)

**展示**：我们将预测序列可视化，以突出FreDF在预测质量上的改进。图3展示了T = 336时ETTm2的一个快照。虽然未使用FreDF的模型能够捕捉标签序列的大致趋势，但难以捕捉序列的高频分量，导致预测结果的频率明显较低。此外，预测序列存在许多毛刺。这些问题反映了在时域中进行预测的局限性，即难以捕捉高频分量以及忽略了序列步骤之间的自相关。FreDF有效地解决了这些局限性。在FreDF下生成的预测不仅能与标签序列同步，准确捕捉高频分量，而且由于考虑了自相关，其外观更平滑，不规则性更少。

### 3.3 消融实验

![20250407113358](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407113358.png)

在本节中，我们剖析时域损失和频域损失对提升预测性能的贡献。结果详见表2，其中使用iTransformer作为预测模型。总体而言，与时域损失相比，频域损失始终能提升性能。其原理在于，标签自相关在频域中能得到有效处理，更符合DF范式中隐含的条件独立假设。此外，在两个域中进行学习，相较于仅依赖单个域，在大多数情况下能带来显著的性能提升。然而，在提升$\mathcal{L}^{(\mathrm{freq})}$方面的改进幅度较小。因此，在多数情况下，单纯聚焦于频域预测是一种可行策略，能在无需平衡复杂学习目标的情况下实现良好性能。

### 4.4 泛化性研究

在本节中，我们探究FreDF在不同预测模型和域变换策略下的效用，以展示FreDF的通用性。在条形图中，预测误差是在预测长度（96、192、336、720）上取平均值，误差条为95%置信区间。

- **不同的预测模型**：我们探索FreDF在增强代表性神经预测模型（iTransformer、DLinear、Autoformer和Transformer ）方面的通用性。FreDF在增强这些模型方面展现出显著效果，使它们相较于传统DF范式有明显提升，如图4所示。值得注意的是，像Autoformer和Transformer这样基于Transformer的模型从FreDF的整合中获益良多。例如，在ECL数据集上，2021年开发的Autoformer通过FreDF实现了性能提升。DLinear（2023年开发）也有类似提升。更多关于FreDF通用性的证据见附录E。这些结果证实了FreDF作为一种即插即用策略，在增强各种时间序列预测模型方面的潜力。
- **不同的FFT实现方式**：我们注意到，标签自相关不仅存在于不同时间步之间，在多变量预测中也存在于不同变量之间。因此，我们实现沿时间（FreDF - T）和变量维度（FreDF - D）的FFT，以处理相应的相关性，结果见表3。一般来说，沿时间轴和变量轴进行FFT能带来相似的性能提升，这表明不同时间步和变量之间存在相关性。特别地，FreDF - T的表现略优于FreDF - D，这凸显了标签序列中自相关的相对重要性。最后，一种策略性方法是将多变量序列视为图像，对时间轴和变量轴（FreDF - 2）都进行二维FFT，这能同时处理时间步和变量之间的相关性，进一步提升性能。

![20250407113650](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407113650.png)

- **不同的变换方式**：鉴于FFT可被视为投影到指数基上，我们通过将FFT替换为投影到其他已确立的多项式上，扩展FreDF的实现方式。每个多项式集都擅长捕捉特定的数据模式，如趋势和周期性，这在时域学习中颇具挑战性。结果总结见图5。值得注意的是，投影到勒让德和傅里叶基上展现出卓越性能。这种优越性源于多项式的正交性，这是附录C中分析的其他方法所不具备的特性。这强调了在选择用于实现FreDF的多项式时，正交性对于消除自相关至关重要。

![20250407113748](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407113748.png)

### 4.5 超参数研究

![20250407114212](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250407114212.png)
