# Timer-XL: Long-Context Transformers for Unified Time Series Forecasting

>领域：时间序列预测、LLM  
>发表在：ICLR 2025  
>模型名字：Timer-XL  
>文章链接：[https://arxiv.org/abs/2410.04803](https://arxiv.org/abs/2410.04803)  
>代码仓库：[https://github.com/thuml/Timer-XL](https://github.com/thuml/Timer-XL)  

## 一、研究背景与问题提出

### 1.1 研究现状

Transformer 已被广泛应用于时间序列预测，成为特定任务模型（Zhou 等人，2021；Wu 等人，2021）和预训练模型（Das 等人，2023）的基础。尽管先前的大多数工作都集中在长期预测上，但可靠的预测是通过考虑Box（2013）所述的内生变化和外生相关性来实现的。此外，预训练 Transformer 的上下文长度决定了推理过程中的最大输入和输出长度。因此，长上下文 Transformer 比短上下文的更具通用性，有助于长序列和高分辨率生成（Yin 等人，2023；Wang 等人，2024a）。

![20250418214456](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250418214456.png)

然而，时间序列领域现有的 Transformer 模型面临着关键的上下文瓶颈问题。如图1所示，与用于自然语言和视觉的 Transformer 不同，后者能够学习数千到数百万个标记之间的依赖关系（Kirillov 等人，2023；OpenAI，2023 ），时间序列 Transformer 通常在最多几百个时间序列标记（块）的有限上下文内运行（Nie 等人，2022）。对于单变量预测，短上下文输入会导致对全局趋势的学习不足，难以处理现实世界时间序列中的非平稳性（Hyndman，2018）。对于多变量预测，越来越多的研究表明，明确捕捉通道内和通道间的依赖关系是有效的（Zhang & Yan，2022；Liu 等人，2023；2024a），这凸显了扩展上下文长度以涵盖相互关联的时间序列的实际紧迫性。

最初基于 Transformer 的预测器主要聚焦于长期预测（Li 等人，2019；Zhou 等人，2021；Wu 等人，2021）。然而，上下文（回溯）长度并未同步增长，这阻碍了 Transformer 做出信息完整的预测。另一项进展聚焦于多变量预测。与自然语言不同，时间序列是多维且内在相关的（Hyndman，2018）。为了学习序列内和序列间的依赖关系，人们提出了时间序列 Transformer 的不同标记化方法，包括逐点（Lim 等人，2021）、逐块（Nie 等人，2022）和逐变量（Liu 等人，2023）方法，以及精心定制的架构（Zhang & Yan，2022；Wang 等人，2024b ）。然而，很少有研究强调多维时间序列可以在不进行架构修改的情况下，通过长上下文 Transformer 统一处理。在这项工作中，我们利用擅长处理长上下文序列的因果 Transformer，并将时间序列预测任务统一为多变量下一个标记预测。

时间序列 Transformer 经历了从小型特定任务模型到预训练大型模型的演变（Das 等人，2023；Woo 等人，2024；Ansari 等人，2024）。其中，仅解码器 Transformer 主要被用作大语言模型的基础架构（Touvron 等人，2023；OpenAI，2023），被视为通用时间序列分析的可扩展选择（Liu 等人，2024c）。通过在监督下独立预测每个标记，仅解码器模型也是多长度预测器（Liu 等人，2024b），避免了资源密集型的训练和回溯搜索。然而，现有的仅解码器 Transformer 通常以通道无关的方式进行预训练，使其无法捕捉序列间的依赖关系。

先前的工作使用仅编码器 Transformer 来捕捉多变量时间序列的依赖关系（Liu 等人，2024a）。然而，我们的实证研究发现，这种架构可能与因果预测不兼容，限制了 Transformer 的性能。为了在单个 Transformer 中实现下一个标记预测和多变量预测，我们改进了注意力模块，将细粒度的标记依赖关系分解为变量依赖关系和时间因果掩码，在保持因果性和可扩展性的同时捕捉序列内和序列间的依赖关系。

## 二、问题剖析与解决策略

![20250418214456](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250418214456.png)

在这项工作中，我们将语言建模的训练目标推广到多变量下一个标记预测，实现了涵盖图1（右）中任务的统一时间序列预测。基于仅解码器架构，我们提出了 TimeAttention，以促进 Transformer 在多维时间序列上的应用，提出了基于克罗内克的掩码机制，以通道相关的方式训练时间序列 Transformer。通过为多变量序列设计专门的位置嵌入，TimeAttention 能够感知时间点的时间顺序，并在变量上实现排列等价性（Zaheer 等人，2017）。我们将上下文扩展到数千个块标记，并在单变量、多变量和协变量信息预测基准上取得了领先性能。通过在大规模数据集上进行预训练，我们提出了 Timer - XL，它是预训练时间序列 Transformer（Timer）的超长版本（Liu 等人，2024c），在零样本预测中优于最近的大型模型。

### 2.1 解决方法

#### 2.1.1 TIMER

Timer（Liu 等人，2024c）是一种通过下一个标记预测进行训练的时间序列 Transformer（Bengio 等人，2000），它将一维时间序列视为不重叠的块标记。

**下一个标记预测**：给定长度为 $TP$ 的单变量时间序列 $\mathbf{X} = \{x_1, \ldots, x_{TP}\}$，一个时间序列标记被定义为 $P$ 个连续的时间点，也称为块标记：
$$\mathbf{x}_i = \{x_{(i - 1)P + 1}, \ldots, x_{iP}\} \in \mathbb{R}^P, \quad i = 1, \ldots, T. \tag{1}$$
训练目标是独立预测下一个块标记，以最大化似然：
$$P(\mathbf{X}) = \prod_{i = 1}^{T} p(\mathbf{x}_{i + 1}|\mathbf{x}_{\leq i}), \tag{2}$$
这是通过具有块数 $L$ 和模型维度 $D$ 的仅解码器架构实现的：
$$\mathbf{h}_i^0 = \mathbf{W}_e \mathbf{x}_i, \quad i = 1, \ldots, T,$$
$$\mathbf{H}^l = \text{TrmBlock}(\mathbf{H}^{l - 1}), \quad l = 1, \ldots, L, \tag{3}$$
$$\{\hat{\mathbf{x}}_{i + 1}\} = \mathbf{H}^L \mathbf{W}_d, \quad i = 1, \ldots, T.$$
为简洁起见，我们省略了块索引 $l$。Timer 使用 $\mathbf{W}_e, \mathbf{W}_d \in \mathbb{R}^{D \times P}$ 来独立嵌入和投影标记嵌入，得到 $\mathbf{H} = \{\mathbf{h}_i\} \in \mathbb{R}^{T \times D}$。TrmBlock 包括前馈网络和带有时间因果掩码 $\mathcal{T} \in \mathbb{R}^{T \times T}$ 的自注意力。$\mathbf{h}_i \in \mathbb{R}^D$ 是前 $i$ 个标记的上下文表示。所有预测的 $\hat{\mathbf{x}}_{i + 1}$ 都通过均方误差（MSE）损失与真实值进行监督训练。

#### 2.1.2 将一维序列扩展到二维时间序列

对于增加了维度的扩展上下文，我们提出的注意力机制旨在：（1）全面捕捉序列内和序列间的依赖关系；（2）在时间维度内保持因果性。在不损失一般性的前提下，我们用多变量预测的例子来说明这一点。

**多变量下一个标记预测**：给定具有变量数 $N$ 的多变量时间序列 $\mathbf{X} \in \mathbb{R}^{N \times TP}$，时间序列标记 $\mathbf{x}_{m, i}$ 被定义为第 $m$ 个变量的第 $i$ 个块：
$$\mathbf{x}_{m, i} = \{x_{m, (i - 1)P + 1}, \ldots, x_{m, iP}\} \in \mathbb{R}^P, \quad m = 1, \ldots, N, \quad i = 1, \ldots, T. \tag{4}$$
训练目标仍然是独立预测下一个标记。与之前不同的是，每次预测都是基于来自所有 $N$ 个变量的前 $i$ 个时间标记：
$$P(\mathbf{X}) = \prod_{m = 1}^{N} \prod_{i = 1}^{T} p(\mathbf{x}_{m, i + 1}|\mathbf{x}_{\leq i}) = \prod_{m = 1}^{N} \prod_{i = 1}^{T} p(\mathbf{x}_{m, i + 1}|\mathbf{x}_{1, \leq i}, \ldots, \mathbf{x}_{N, \leq i}). \tag{5}$$
与公式（2）相比，多变量长度从 $T$ 增加到 $NT$。相比之下，这种范式的优点是在纳入来自其他序列的外生变量相关性的同时，学习每个序列内的因果依赖关系，从而创建一个通用的预测范式，实现通道独立（Nie 等人，2022）或变量中心模型（Liu 等人，2023）。

从技术上讲，我们对每个标记独立应用 $\mathbf{W}_e \in \mathbb{R}^{D \times P}$，以获得逐块表示 $\mathbf{h}_{m, i} \in \mathbb{R}^D$，它将通过 Transformer 块涵盖来自 $Ni$ 个标记的上下文信息，并最终由 $\mathbf{W}_d \in \mathbb{R}^{D \times P}$ 投影到预测的块标记 $\hat{\mathbf{x}}_{m, i + 1}$ 中。

**位置嵌入**：在时间序列 Transformer 中，位置嵌入尚未得到充分探索。为了避免自注意力中的排列不变性，需要位置嵌入来反映标记在时间维度上的时间顺序。对于变量维度，打乱变量的输入顺序除了会影响变量的输出顺序外，不应影响其他任何内容。形式上，对多个变量的处理应该是排列等价的（Zaheer 等人，2017）。

为了满足上述要求，我们采用 [RoPE](https://zhuanlan.zhihu.com/p/642884818)，这是一种广泛使用的位置嵌入方法，用于时间维度。对于变量维度，我们使用可学习标量，每个头一个，以保持变量的排列等价性（Woo 等人，2024）。除了简单地将它们结合起来，我们在章节 E.3 中提供了详细的公式，以证明其有效性：
$$\boldsymbol{A}_{mn, ij} = \mathbf{h}_{m, i}^{\top} \mathbf{W}_q \boldsymbol{\Rho}_{q, j - i} \mathbf{W}_k^{\top} \mathbf{h}_{n, j} + u \cdot \mathbb{1}(m = n) + v \cdot \mathbb{1}(m \neq n), \tag{6}$$
其中 $\mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v \in \mathbb{R}^{D \times d_k}$，$d_k$ 是查询、键和值的维度。$\boldsymbol{\Rho}_{q, t} \in \mathbb{R}^{d_k \times d_k}$ 是旋转度为 $t \cdot \theta$ 的旋转矩阵，$\mathbb{1}(\cdot)$ 是指示函数，$u, v \in \mathbb{R}$ 是用于区分内生和外生因果块的可学习参数。

**TimeAttention**：与基于变量的（Liu 等人，2023）和非因果逐块标记（Nie 等人，2022；Woo 等人，2024）不同，我们的 TimeAttention 旨在捕捉所有变量内部和之间的因果逐块依赖关系。具体来说，我们将二维索引展平为一维索引，按时间顺序对块标记进行排序，如图 2 左侧所示。请注意，变量的顺序无关紧要，因为公式（6）保证了它们的排列等价性。

![20250420161220](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420161220.png)

我们提供一个直观的例子来说明多变量时间序列中的因果依赖关系：考虑时间序列 A 的第二个标记。为了预测其下一个标记，其表示 $\mathbf{h}$ 应该恰好依赖于标记 $\{1, 2, 4, 5\}$。同样，我们在图 12 中提供了每个标记的所有因果依赖关系。基于可视化注意力掩码和图 2 中呈现的所有变量依赖关系（其中所有变量都是相互关联的），A 中的所有标记依赖关系可以形式上分解为变量依赖关系图 $\mathcal{C} \in \mathbb{R}^{N \times N}$ 和因果时间掩码 $\mathcal{T} \in \mathbb{R}^{T \times T}$ 的[克罗内克积](https://zh.wikipedia.org/wiki/%E5%85%8B%E7%BD%97%E5%86%85%E5%85%8B%E7%A7%AF) (1) 邻接矩阵：
$$\mathcal{T}_{i, j} = \begin{cases} 1 & \text{如果 } j \leq i, \\ 0 & \text{否则,} \end{cases} \quad \mathcal{C}_{m, n} = \begin{cases} 1 & \text{如果变量 } m \text{ 依赖于 } n, \\ 0 & \text{否则.} \end{cases} \tag{7}$$
设克罗内克积 $\otimes : (\mathbb{R}^{N \times N}, \mathbb{R}^{T \times T}) \mapsto \mathbb{R}^{NT \times NT}$ 取两个矩阵并生成一个块矩阵。因此，TimeAttention 如下：
$$\text{TimeAttention}(\mathbf{H}) = \text{Softmax}\left( \frac{\text{Mask}(\mathcal{C} \otimes \mathcal{T}) + \boldsymbol{A}}{\sqrt{d_k}} \right) \mathbf{H} \mathbf{W}_v, \quad \text{Mask}(\mathcal{M})_{ij} = \begin{cases} 0 & \text{如果 } \mathcal{M}_{i, j} = 1, \\ -\infty & \text{如果 } \mathcal{M}_{i, j} = 0. \end{cases} \tag{8}$$

最终，$\mathbf{H} = \{\mathbf{h}_{m, i}\} \in \mathbb{R}^{NT \times D}$ 中的标记表示将由前馈网络和层归一化独立处理，并输入到下一个 Transformer 块中。

![20250420161220](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420161220.png)

**统一时间序列预测**：在多变量预测中，变量依赖关系形成完整图，呈现全一矩阵 $\mathcal{C}$。通过在多个序列上推广 TimeAttention，Transformer 可以利用其长度灵活性来涵盖相关的协变量。在这种情况下，Timer - XL 分两步进行调整：（1）将定制的变量依赖关系表示为 $\mathcal{C}$；（2）使用目标变量的监督来优化模型。例如（目标 - A - 协变量 - B），右侧图 2 说明了 TimeAttention 如何适应时间和变量维度的位置嵌入。为了实现统一的时间序列预测，我们将二维时间序列展平为统一的上下文，并捕捉细粒度的因果标记依赖关系。

## 三、实验验证与结果分析

我们从三个方面对Timer - XL进行评估，包括：（1）作为特定任务预测器的监督训练；（2）作为零样本预测器的大规模预训练；（3）评估时间注意力（TimeAttention）的有效性和模型效率。鉴于长上下文预测范式在该领域较少受到关注，这可能是由于先前基准测试中的性能饱和所掩盖（Makridakis等人，2020；Wu等人，2022），我们建立了新的长上下文预测基准。详细的实验配置见附录B。

### 3.1 单变量时间序列预测

**设置**：由于在单变量数据集中扩展上下文时数据集长度不足（Makridakis等人，2020），我们采用了Liu等人（2023）的多变量数据集。尽管这些数据集原本是多变量的，但通过实现通道独立性，旨在以单变量方法进行预测。与先前的长期预测设置不同，我们专注于基于长上下文的可靠预测。因此，我们固定预测范围，并将回溯长度增加到月度和年度水平。我们还基于具有挑战性的40年欧洲中期天气预报中心再分析v5数据集（Hersbach等人，2020）建立了一个长上下文单变量基准，其中使用年度上下文来预测单个站点的地表温度（ERA5 - S）。

![20250420165338](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420165338.png)

**结果**：如图3所示，将每日上下文扩展到月度，通常可以提高单变量预测的准确性。在ERA5数据集上（表15）我们也得出了类似结论，扩展上下文对特定模型架构始终有帮助。值得注意的是，仅解码器架构的Timer - XL在极长的上下文中优于仅编码器的Transformer和线性预测器。此外，我们在附录E.4中进行了表示分析，结果表明Timer - XL擅长在大量观测数据中自适应选择信息，从而取得突破性性能。同样值得注意的是，月度和年度上下文的性能提升缓慢且会恶化，这可能源于数据中固有的噪声增加和训练难度增大，这为提高上下文效率留下了未来的研究方向。表2提供了ERA5 - S数据集上的结果。Timer - XL在所有站点上始终优于PatchTST，这可归因于仅解码器架构中对因果关系的维护和逐个标记监督。

![20250420165920](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420165920.png)

**非平稳预测**：我们深入研究了单变量任务中普遍存在的非平稳性。在先前的基准测试中，通常通过归一化来解决此问题（Kim等人，2021），这极大地提高了Transformer的性能。然而，我们发现这可能是由于这些数据集中时间跨度和训练样本不足造成的。虽然归一化通过将具有不同均值和方差的序列调整到相同分布来简化学习，但它限制了Transformer的模型容量，使其无法学习窗口之间的变化。其副作用可能是模式崩塌和过度平滑的预测。在表2和表16中，我们评估了在ERA5数据集以及Wu等人（2022）的数据集中的性能，验证了即使不进行实例归一化，Timer - XL也能取得更好的结果。

#### 4.2 多变量时间序列预测

**设置**：我们遵循iTransformer（Liu等人，2023）来评估多变量预测性能。为了构建通用预测器，我们评估滚动预测的性能，即通过将前一次预测整合到下一次迭代的回溯窗口中，为所有预测范围训练一个模型。我们进一步建立了长上下文多变量预测基准：ERA5多站点地表温度预测（ERA5 - MS）和全球温度与风速预测挑战（GTWSF）（Wu等人，2023），以便通过足够的训练样本学习复杂的时间动态和变量相关性。

![20250420170313](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420170313.png)

![20250420170324](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420170324.png)

**结果**：如表3、表4和图4所示，Timer - XL在先前和新的基准测试中均取得了最佳结果。本质上，像UniTST（Liu等人，2024a）和iTransformer这样明确捕捉序列间依赖关系的Transformer，在表3中也取得了不错的性能。除了iTransformer，Timer - XL还可以对细粒度的逐块时间依赖关系进行建模。借助TimeAttention，Timer - XL在高维时间序列上的表现尤其优于Timer（在ECL数据集中提高了13.2%，在Traffic数据集中提高了6.3%，上下文中有数千个标记）。与仅编码器的UniTST相比，仅解码器的Transformer在表4中不同预测长度上的泛化能力更强。

![20250420170626](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420170626.png)

**消融研究**：分块（Nie等人，2022）已被证明是一种有效的时间序列标记化方法，推动了Transformer在监督深度预测器和大型时间序列模型中的蓬勃发展。为了更好地应对多变量时间序列预测，我们在实际基准测试中比较了典型模型，以解决关键问题：（1）是否进行显式序列间建模（通道独立性）；（2）使用仅解码器还是仅编码器的Transformer。表5展示了四种Transformer的组合，结果表明Timer - XL结合了显式序列间建模和仅解码器架构的优势，适用于有足够训练样本的多变量时间序列预测。

#### 4.3 协变量相关时间序列预测

**设置**：对于协变量相关的预测，我们采用公认的电价预测（EPF）任务（Lago等人，2021）。每个子集包含电价作为内生变量和两个外生变量。因此，Timer - XL的变量依赖关系表示为$\mathcal{C} = [[1, 1, 1], [0, 1, 0], [0, 0, 1]]$ 。为了研究在协变量中学习因果还是非因果逐块依赖关系，我们实现了Timer - XL的两个版本：原始版本带有时间因果掩码$\mathcal{T}$ ，非因果版本则将$\mathcal{T}$ 替换为全一矩阵。

![20250420170937](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420170937.png)

**结果**：如表6所示，Timer - XL在协变量相关任务中优于现有先进模型。与TimeXer（Wang等人，2024b）将整个协变量视为一个标记不同，Timer - XL学习细粒度的逐块依赖关系。通过Timer - XL的非因果版本，我们意外地发现与内生变量一致的结论：如果Timer - XL学习外生变量中的因果依赖关系，结果会更好。这再次验证了保持因果关系的下一个标记预测具有更高的性能上限。

#### 4.4 预训练时间序列Transformer

**设置**：预训练为时间序列Transformer赋予了可泛化的预测能力。大规模时间序列模型的成果可以应对少样本和零样本预测中的广泛挑战。在本节中，我们在UTSD（Liu等人，2024c）和LOTSA（Woo等人，2024）上进行单变量预训练，并在Wu等人（2022）的基准测试中评估零样本性能。我们进一步在我们的ERA5 - Large数据集上进行大规模多变量预训练，该数据集涵盖40年时间，包含4920个站点。随后，我们评估了PatchTST（仅编码器Transformer）和Timer - XL（仅解码器Transformer）三种泛化结果：预训练中80%站点和80%时间跨度的剩余部分（变量泛化）、剩余时间跨度（时间泛化）以及剩余的时间跨度和站点划分（变量和时间泛化）。为了评估更长上下文预训练的益处，我们比较了Timer（2024c）和Timer - XL的零样本性能，其中预训练的上下文长度从1440增加到2880。

**结果**：我们在图5（a）中间部分比较了ERA5 - Large数据集上的泛化性能。在所有情况下，Timer - XL的结果都优于PatchTST，表明仅解码器架构具有更强的泛化能力。图5（b）比较了两个具有不同上下文长度的预训练Transformer的零样本性能，其中Timer - XL在所有基准数据集上的表现都优于之前的Timer，验证了长上下文预训练可以增强大型时间序列模型。在表7中，我们在可比的预训练规模和模型大小下提供了全面的零样本评估，其中Timer - XL以更高的样本效率取得了显著性能。其多功能性和可扩展性使其成为基础模型有前途的骨干架构。

#### 4.5 模型效率

![20250420172713](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420172713.png)

为了评估Timer - XL相对于上下文长度的模型效率，有必要认识到时间序列数据与一维序列相比的独特特征。与自然语言不同，时间序列模态的特点是变量数量$N$和输入长度可变 。我们采用两个具有不同$N$值的代表性多变量数据集，并在逐渐延长的输入情况下提供内存占用和训练速度数据。我们将评估处理多变量时间序列的典型方法：（1）Timer - XL和Moira，它们假定通道相关性；（2）Timer，它采用通道独立性。直观地说，第一种类型的复杂度为$\mathcal{O}(N^{2}T^{2})$ ，而通道独立下自注意力的复杂度为$\mathcal{O}(NT^{2})$ 。然而，图6所示的结果表明，Timer - XL的实测开销远小于Timer的$N$倍。

由于之前对时间序列Transformer模型效率的分析主要集中在一维时间序列的自注意力上，我们首先给出二维时间序列上Transformer计算复杂度的理论推导，包括参数数量、内存占用和浮点运算次数，见表8。我们发现，Transformer的其他部分，如前馈网络，无论采用何种方法处理多变量时间序列，其复杂度均为$\mathcal{O}(NT)$ 。在现有基准测试中，由于上下文长度不够长，它们也构成了主要开销，这证实了我们的实证结果。

#### 4.6 表示分析

![20250420172806](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250420172806.png)

除了性能提升之外，细粒度的标记依赖关系还提高了可解释性。我们在图7中展示了来自Traffic数据集的可视化示例。可以观察到，沿对角线的子矩阵通常会受到更多关注，这合理地揭示了内生变量内的主要依赖关系。通过放大对应于变量3的子块，我们观察到最后一行的注意力分布可以表明块标记之间存在某些强依赖关系。自相关函数图（ACF）也支持这一观察结果，它揭示了具有一定滞后的自相关，因此模型会特别关注这些标记。此外，我们将每个子矩阵平均为一个标量。得到的矩阵也可以说明原始数据中呈现的皮尔逊相关性。
