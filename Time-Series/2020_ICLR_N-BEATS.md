# N-BEATS: Neural basis expansion analysis for interpretable time series forecasting

>领域：时间序列预测  
>发表在：ICLR 2020  
>模型名字：**N**eural **b**asis **e**xpansion **a**nalysis for interpretable **t**ime **s**eries forecasting  
>文章链接：[N-BEATS: Neural basis expansion analysis for interpretable time series forecasting](https://arxiv.org/abs/1905.10437)  
>代码仓库：[sktime/nbeats](https://github.com/sktime/pytorch-forecasting/blob/main/pytorch_forecasting/models/nbeats/_nbeats.py)  
![20250408142910](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250408142910.png)

## 一、研究背景与问题提出

### 1.1 研究现状

时间序列（TS）预测是一个重要的商业问题，也是机器学习（ML）富有成效的应用领域。它是现代商业大多数方面的基础，包括库存控制、客户管理等关键领域，以及从生产、分销到财务和营销的商业规划。因此，它对财务有重大影响，预测准确性每提高一点，往往就能带来数百万美元的收益（Jain，2017；Kahn，2003）。
Dickey-Fuller检验
然而，与计算机视觉或自然语言处理等深度学习（DL）技术已广泛应用的领域不同，仍有证据表明，机器学习和深度学习在超越经典统计时间序列预测方法方面存在困难（Makridakis等人，2018a、b）。例如，在提交给M4竞赛的六种 “纯” 机器学习方法中，在总共60个参赛方法中排名分别为23、37、38、48、54和57，而排名靠前的方法大多是经典统计技术的集成（Makridakis等人，2018b）。

### 1.2 引出问题

另一方面，M4竞赛的获胜者（Smyl，2020），是基于神经残差/注意力扩张长短期记忆网络（LSTM）堆栈与具有可学习参数的经典Holt - Winters统计模型（Holt，1957；2004；Winters，1960）的混合模型。由于Smyl的方法在很大程度上依赖于Holt - Winters组件，Makridakis等人（2018b）进一步认为 “混合方法和方法组合是提高预测准确性、提升预测价值的前进方向”。在这项工作中，我们希望通过探索纯深度学习架构在时间序列预测中的潜力来挑战这一结论。此外，在可解释深度学习架构设计的背景下，我们对回答以下问题感兴趣：我们能否在模型中引入合适的归纳偏差，使其内部操作更具可解释性，即提取一些可解释的驱动因素来产生给定的预测？

## 二、问题剖析与解决策略

### 2.1 问题剖析

我们考虑离散时间下的单变量点预测问题。给定长度为$T$的观测序列历史$y_1,\dots,y_T \in \mathbb{R}^T$ ，任务是预测长度为$H$ 的未来值向量$\mathbf{y} \in \mathbb{R}^H$  ，即$[y_{T + 1}, y_{T + 2},\dots, y_{T + H}]$ 。为简化起见，我们稍后将考虑一个长度为$t \leq T$ 、以最后一个观测值$y_T$ 结束的回望窗口作为模型输入，记为$\mathbf{x} \in \mathbb{R}^t = [y_{T - t + 1},\dots,y_T]$ 。我们将$\mathbf{y}$ 的预测记为$\hat{\mathbf{y}}$ 。以下指标常用于评估预测性能（Hyndman & Koehler，2006；Makridakis & Hibon，2000；Makridakis等人，2018b；Athanasopoulos等人，2011）：

- **对称平均绝对百分比误差（sMAPE）**：$sMAPE = \frac{200}{H}\sum_{i = 1}^{H}\frac{|y_{T + i} - \hat{y}_{T + i}|}{|y_{T + i}| + |\hat{y}_{T + i}|}$
- **平均绝对百分比误差（MAPE）**：$MAPE = \frac{100}{H}\sum_{i = 1}^{H}\frac{|y_{T + i} - \hat{y}_{T + i}|}{|y_{T + i}|}$
- **平均绝对缩放误差（MASE）**：$MASE = \frac{1}{H}\sum_{i = 1}^{H}\frac{|y_{T + i} - \hat{y}_{T + i}|}{\frac{1}{T + H - m}\sum_{j = T + m}^{T + H}|y_j - y_{j - m}|}$
- **总体加权平均（OWA）**：$OWA = \frac{1}{2}\left[\frac{sMAPE}{sMAPE_{Naive2}} + \frac{MASE}{MASE_{Naive2}}\right]$

这里$m$ 是数据的周期性（例如，月度序列为12）。MAPE（平均绝对百分比误差）、sMAPE（对称平均绝对百分比误差）和MASE（平均绝对缩放误差）是实践中常用的无尺度预测指标（Hyndman & Koehler，2006；Makridakis & Hibon，2000）：sMAPE通过预测值与真实值的平均偏差进行缩放，MASE通过朴素预测器（简单复制过去$m$ 个周期观测值）的平均误差进行缩放，从而考虑了季节性。OWA（总体加权平均）是M4竞赛特有的用于排名参赛队伍的指标（M4 team，2018），其中sMAPE和MASE指标经过归一化处理，使季节性调整后的朴素预测器的OWA值为1.0。

### 2.1 解决方法

我们的架构设计方法基于几个关键原则。首先，基础架构应简单、通用且具有较强的表达能力（深度）。其次，该架构不应依赖于特定于时间序列的特征工程或输入缩放。这些先决条件使我们能够探索纯深度学习架构在时间序列预测中的潜力。最后，作为探索可解释性的前提，该架构应可扩展，使其输出便于人类理解。我们现在讨论这些原则如何促成所提出的架构。

#### 2.1.1 基本模块

![20250408142910](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250408142910.png)

所提出的基本构建模块采用分叉架构，如图1（左）所示。在本节中，我们将详细描述第$\ell$个模块的操作（为简洁起见，在图1中省略了模块索引$\ell$ ）。第$\ell$个模块接受其相应的输入$\mathbf{x}_\ell$ ，并输出两个向量$\hat{\mathbf{x}}_\ell$ 和$\hat{\mathbf{y}}_\ell$ 。对于模型中的第一个模块，其相应的$\mathbf{x}_\ell$ 是整个模型的输入，即具有一定长度、以最后一次测量观测值结束的历史回望窗口。我们将输入窗口的长度设置为预测范围$H$ 的倍数，在我们的设置中，$\mathbf{x}$ 的典型长度范围从$2H$ 到$7H$ 。对于其余模块，其输入$\mathbf{x}_\ell$ 是前一个模块的残差输出。每个模块有两个输出：$\hat{\mathbf{y}}_\ell$ ，即该模块长度为$H$ 的前向预测；以及$\hat{\mathbf{x}}_\ell$ ，即该模块在功能空间约束下对$\mathbf{x}_\ell$ 的最佳估计，也称为 “后向预测”，该功能空间是模块可用于逼近信号的空间。

在内部，基本构建模块由两部分组成。第一部分是一个全连接网络，它产生前向 $\theta^f_\ell$ 和后向 $\theta^b_\ell$ 扩展系数预测器（同样，为简洁起见，在图1中对$\theta^b$、$\theta^f$、$g^b$、$g^f$省略了模块索引$\ell$ ）。第二部分由后向$g^b_\ell$ 和前向$g^f_\ell$ 基函数层组成，它们接受相应的前向 $\theta^f_\ell$ 和后向 $\theta^b_\ell$ 扩展系数，根据一组基函数进行计算，并产生后向预测$\hat{\mathbf{x}}_\ell$ 和前向预测输出$\hat{\mathbf{y}}_\ell$ ，具体如前文所述。

第$\ell$个模块第一部分的操作由以下等式描述：
$$
\begin{align}
\mathbf{h}_{\ell,1} &= \mathrm{FC}_{\ell,1}(\mathbf{x}_\ell), &\mathbf{h}_{\ell,2} &= \mathrm{FC}_{\ell,2}(\mathbf{h}_{\ell,1}), \\
\mathbf{h}_{\ell,3} &= \mathrm{FC}_{\ell,3}(\mathbf{h}_{\ell,2}), &\mathbf{h}_{\ell,4} &= \mathrm{FC}_{\ell,4}(\mathbf{h}_{\ell,3}). \\
\theta^b_\ell &= \mathrm{LINEAR}^b_\ell(\mathbf{h}_{\ell,4}), &\theta^f_\ell &= \mathrm{LINEAR}^f_\ell(\mathbf{h}_{\ell,4}).
\end{align}
$$
这里的LINEAR层只是一个线性投影层，即$\theta^f_\ell = \mathbf{W}^f_\ell\mathbf{h}_{\ell,4}$ 。FC层是具有ReLU非线性的标准全连接层（Nair & Hinton，2010），例如对于$\mathrm{FC}_{\ell,1}$ ，我们有：$\mathbf{h}_{\ell,1} = \mathrm{ReLU}(\mathbf{W}_{\ell,1}\mathbf{x}_\ell + \mathbf{b}_{\ell,1})$ 。这部分架构的一个任务是预测前向扩展系数$\theta^f_\ell$ ，最终目标是通过适当地混合$g^f_\ell$ 提供的基向量来优化部分预测$\hat{\mathbf{y}}_\ell$ 的准确性。此外，这个子网络预测后向扩展系数$\theta^b_\ell$ ，$g^b_\ell$ 使用它来生成$\mathbf{x}_\ell$ 的估计值，最终目标是通过去除输入中对预测无帮助的成分来帮助下游模块进行预测。

网络的第二部分通过基函数层将扩展系数$\theta^f_\ell$ 和$\theta^b_\ell$ 映射到输出，$\hat{\mathbf{y}}_\ell = g^f_\ell(\theta^f_\ell)$ 且$\hat{\mathbf{x}}_\ell = g^b_\ell(\theta^b_\ell)$ 。其操作由以下等式描述：
$$
\hat{\mathbf{y}}_\ell = \sum_{i = 1}^{\mathrm{dim}(\theta^f_\ell)} \theta^f_{\ell,i} \mathbf{v}^f_i, \quad
\hat{\mathbf{x}}_\ell = \sum_{i = 1}^{\mathrm{dim}(\theta^b_\ell)} \theta^b_{\ell,i} \mathbf{v}^b_i.
$$
这里$\mathbf{v}^f_i$ 和$\mathbf{v}^b_i$ 是预测和后向预测基向量，$\theta^f_{\ell,i}$ 是$\theta^f_\ell$ 的第$i$个元素。$g^b_\ell$ 和$g^f_\ell$ 的功能是提供足够丰富的集合$\{\mathbf{v}^f_i\}_{i = 1}^{\mathrm{dim}(\theta^f_\ell)}$ 和$\{\mathbf{v}^b_i\}_{i = 1}^{\mathrm{dim}(\theta^b_\ell)}$ ，使得它们各自的输出可以通过变化的扩展系数$\theta^f_\ell$ 和$\theta^b_\ell$ 充分表示。如下所示，$g^b_\ell$ 和$g^f_\ell$ 可以选择为可学习的，也可以设置为特定的函数形式，以反映特定问题的归纳偏差，从而适当地约束输出结构。$g^b_\ell$ 和$g^f_\ell$ 的具体示例将在3.3节中讨论。

#### 2.1.2 双重残差堆叠

![20250408142910](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250408142910.png)

经典的残差网络架构在将层堆叠的输出传递到下一个堆叠之前，会将层堆叠的输入添加到输出中（He等人，2016）。Huang等人（2017）提出的DenseNet架构通过从每个堆叠的输出到其后的每个其他堆叠的输入添加额外连接来扩展这种方法。这些方法在提高深度架构的可训练性方面具有明显优势。在这项工作的背景下，它们的缺点是会导致难以解释的网络结构。我们提出了一种新颖的分层双重残差拓扑结构，如图1（中、右）所示。所提出的架构有两个残差分支，一个在每层的后向预测上运行，另一个在每层的预测分支上运行。其操作由以下等式描述：
$$
\mathbf{x}_\ell = \mathbf{x}_{\ell - 1} - \hat{\mathbf{x}}_{\ell - 1}, \quad
\hat{\mathbf{y}} = \sum_{\ell} \hat{\mathbf{y}}_\ell.
$$
如前所述，在第一个模块的特殊情况下，其输入是模型级别的输入$\mathbf{x}$，$\mathbf{x}_1 \equiv \mathbf{x}$ 。对于所有其他模块，前向残差分支$\mathbf{x}_\ell$ 可以看作是输入信号的顺序分析。前一个模块去除它能够很好逼近的信号部分$\hat{\mathbf{x}}_{\ell - 1}$ ，这使得下游模块的预测工作更轻松。这种结构也有利于更流畅的梯度反向传播。更重要的是，每个模块输出一个部分预测$\hat{\mathbf{y}}_\ell$ ，首先在堆叠级别聚合，然后在整体网络级别聚合，从而提供分层分解。最终预测$\hat{\mathbf{y}}$ 是所有部分预测的总和。在通用模型的背景下，当允许每个堆叠层具有任意的$g^b_\ell$ 和$g^f_\ell$ 时，这使得网络的梯度流更加透明。在$g^b_\ell$ 和$g^f_\ell$ 具有特定结构的特殊情况下，这表明了通过聚合有意义的部分预测实现可解释性的关键重要性。

#### 2.1.3 可解释性

基于$g^b_\ell$ 和$g^f_\ell$ 的选择，我们提出了两种架构配置。一种是通用深度学习架构，另一种通过添加特定的归纳偏差来实现可解释性。

**通用架构**：不依赖于特定于时间序列的知识。我们将$g^b_\ell$ 和$g^f_\ell$ 设置为前一层输出的线性投影。在这种情况下，模块$\ell$ 的输出描述如下：
$$
\hat{\mathbf{y}}_\ell = \mathbf{V}^f_\ell \theta^f_\ell + \mathbf{b}^f_\ell, \quad
\hat{\mathbf{x}}_\ell = \mathbf{V}^b_\ell \theta^b_\ell + \mathbf{b}^b_\ell.
$$
对该模型的解释是，图1中所示基本构建模块中的全连接层在网络学习到的基$\mathbf{V}^f_\ell$ 中学习部分预测$\hat{\mathbf{y}}_\ell$ 的预测性分解。矩阵$\mathbf{V}^f_\ell$ 的维度为$H \times \mathrm{dim}(\theta^f_\ell)$ 。因此，$\mathbf{V}^f_\ell$ 的第一维在预测域中表示离散时间索引。矩阵的第二维表示此基中的索引，即$\theta^f_\ell$ 中扩展系数的索引。因此，$\mathbf{V}^f_\ell$ 的列可以看作是时域中的波形。由于对$\mathbf{V}^f_\ell$ 的形式没有施加额外约束，深度模型学习到的波形没有固有结构（在我们的实验中也没有明显结构）。这导致$\hat{\mathbf{y}}_\ell$ 不具有可解释性。

**可解释架构**：可以通过重用图1中的整体架构并向基函数层添加结构来构建。预测从业者经常将时间序列分解为趋势和季节性，例如STL（Cleveland等人，1990）和X - 13 - ARIMA（美国人口普查局，2013）所执行的那样。我们建议在模型中设计趋势和季节性分解，以使堆叠输出更易于解释。请注意，对于通用模型，堆叠的概念不是必需的，为清晰起见省略了堆叠级别索引。现在我们将同时考虑堆叠级别和模块级别索引。例如，$\hat{\mathbf{y}}_{s,\ell}$ 将表示堆叠$s$ 内模块$\ell$ 的部分预测。

- **趋势模型**：趋势的典型特征是它在大多数情况下是单调函数，或者至少是缓慢变化的函数。为了模拟这种行为，我们建议将$g^b_{s,\ell}$ 和$g^f_{s,\ell}$ 约束为低次$p$ 的多项式，这是一种在预测窗口上缓慢变化的函数：

$$
\hat{\mathbf{y}}_{s,\ell} = \sum_{i = 0}^{p} \theta^f_{s,\ell,i} t^i.
$$
这里时间向量$\mathbf{t} = [0, 1, 2,\dots, H - 2, H - 1]^T / H$ 定义在从$0$ 到$(H - 1) / H$ 的离散网格上，用于提前$H$ 步预测。或者，趋势预测的矩阵形式为：
$$
\hat{\mathbf{y}}^{tr}_{s,\ell} = \mathbf{T} \theta^f_{s,\ell},
$$
其中$\theta^f_{s,\ell}$ 是由等式(1)描述的堆叠$s$ 的层$\ell$ 的全连接网络预测的多项式系数；$\mathbf{T} = [\mathbf{1}, \mathbf{t},\dots, \mathbf{t}^p]$ 是$\mathbf{t}$ 的幂矩阵。如果$p$ 较低，例如$2$ 或$3$ ，它会迫使$\hat{\mathbf{y}}^{tr}_{s,\ell}$ 模拟趋势。

- **季节性模型**：季节性的典型特征是它是规则的、周期性的、反复出现的波动。因此，为了对季节性进行建模，我们建议将$g^b_{s,\ell}$ 和$g^f_{s,\ell}$ 约束为属于周期函数类，即$y_t = y_{t - \Delta}$ ，其中$\Delta$ 是季节周期。对周期函数进行建模的基函数的自然选择是傅里叶级数：

$$
\hat{\mathbf{y}}_{s,\ell} = \sum_{i = 0}^{\lfloor H / 2 - 1 \rfloor} \theta^f_{s,\ell,i} \cos(2 \pi i t) + \theta^f_{s,\ell,i + \lfloor H / 2 \rfloor} \sin(2 \pi i t),
$$
那么季节性预测将具有以下矩阵形式：
$$
\hat{\mathbf{y}}^{seas}_{s,\ell} = \mathbf{S} \theta^f_{s,\ell},
$$
其中$\theta^f_{s,\ell}$ 是由等式(1)描述的堆叠$s$ 的层$\ell$ 的全连接网络预测的傅里叶系数；$\mathbf{S} = [\mathbf{1}, \cos(2 \pi \mathbf{t}),\dots, \cos(2 \pi \lfloor H / 2 - 1 \rfloor \mathbf{t}), \sin(2 \pi \mathbf{t}),\dots, \sin(2 \pi \lfloor H / 2 - 1 \rfloor \mathbf{t})]$ 是正弦波形矩阵。预测$\hat{\mathbf{y}}^{seas}_{s,\ell}$ 是一个模仿典型季节模式的周期函数。

整体可解释架构由两个堆叠组成：趋势堆叠之后是季节性堆叠。双重残差堆叠与预测/后向预测原则相结合，导致（i）在将输入窗口$\mathbf{x}$ 输入到季节性堆叠之前，从其中去除趋势成分；（ii）趋势和季节性的部分预测作为单独的可解释输出可用。从结构上讲，每个堆叠由多个模块组成，这些模块通过残差连接相互连接，如图1所示，并且每个模块共享其相应的不可学习的$g^b_{s,\ell}$ 和$g^f_{s,\ell}$ 。趋势和季节性堆叠的模块数量均为$3$ 。我们发现，除了共享$g^b_{s,\ell}$ 和$g^f_{s,\ell}$ 之外，在一个堆叠中的模块之间共享所有权重会带来更好的验证性能。

#### 2.1.4 集成

在M4竞赛中，所有排名靠前的参赛作品都使用了集成方法。为了具有可比性，我们也依赖集成方法。我们发现，集成是一种比常用的替代方法（如丢弃法或L2范数惩罚）强大得多的正则化技术。添加这些方法可以改进单个模型，但会损害集成模型的性能。集成的核心属性是多样性。我们使用多个具有多样性的模型构建集成。首先，集成模型在三个不同的指标上进行拟合：sMAPE、MASE和MAPE，其中sMAPE是一种仅在分母中包含真实值的sMAPE版本。其次，对于每个预测范围$H$ ，单个模型在不同长度的输入窗口上进行训练：$2H$、$3H$、…、$7H$ ，总共有六种窗口长度。因此，整体集成表现出多尺度特性。最后，我们通过纳入使用不同随机初始化训练的模型来执行bagging过程（Breiman，1996）。我们总共使用180个模型来报告测试集上的结果（有关集成规模的消融实验，请参阅附录B）。我们使用中位数作为集成聚合函数。

## 三、实验验证与结果分析

### 3.1 基于多个数据集综合性能指标的关键实证结果

我们基于多个数据集（M4、M3和TOURISM）综合性能指标得出的关键实证结果列于表1。对这些数据集更详细的描述见5.1节和附录A。对于每个数据集，我们依据适用于各数据集的常用指标（M4：OWA和sMAPE；M3：sMAPE；TOURISM：MAPE），将我们的结果与文献中该数据集排名前五的结果进行比较。更细致的、按预测范围和时间序列类型划分的特定数据集结果见相应附录（M4：附录C.1；M3：附录C.2；TOURISM：附录C.3 ）。

![20250418201502](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250418201502.png)

在表1中，我们研究了两种N - BEATS配置（通用型（N - BEATS - G）和可解释型（N - BEATS - I））以及N - BEATS - I + G（N - BEATS - G和N - BEATS - I所有模型的集成）的性能。在M4数据集上，我们与M4竞赛的五个代表模型进行比较：每个都是其所属模型类别的最佳模型。Pure ML是B. Trotta的提交成果，是六个纯机器学习模型中的最佳模型。Statistical是N.Z. Legaki和K. Koutsouri提出的最佳纯统计模型。ML/TS combination是P. Montero - Manso、T. Talagala、R.J. Hyndman和G. Athanasopoulos提出的模型，排名第二，是基于几个统计时间序列模型的梯度提升树。ProLogistica是M4中基于统计方法加权集成的第三个模型。最后，DL/TS Hybrid是M4竞赛的获胜模型（Smyl，2020）。

在M3数据集上，我们与Theta方法（Assimakopoulos和Nikolopoulos，2000）——M3的获胜方法、DOTA（一种动态优化的Theta模型（Fiorucci等人，2016））、EXP（最新的统计方法，也是M3此前的先进方法（Spiliotis等人，2019））以及ForecastPro（一种基于指数平滑、ARIMA和移动平均模型选择的现成预测软件（Athanasopoulos等人，2011；Assimakopoulos和Nikolopoulos，2000））进行比较。

在TOURISM数据集上，我们与三个统计基准进行比较（Athanasopoulos等人，2011）：Exponential smoothing（交叉验证的加法/乘法模型）、Theta method、ForePro（与M3中的ForecastPro相同），以及来自TOURISM Kaggle竞赛的前两名模型（Athanasopoulos和Hyndman，2011）：Stratometrics（一种未知技术）；LeeCBaker（Baker和Howard，2011），它是朴素模型、线性趋势模型和指数加权最小二乘回归趋势的加权组合。

根据表1，N - BEATS在三个具有挑战性且不重叠的数据集上展现出了先进的性能，这些数据集包含来自不同领域、采样频率和季节性的时间序列。例如，在M4数据集上，N - BEATS获胜模型与M4获胜模型（0.822 - 0.795 = 0.026）之间的OWA差距大于M4第四名与第二名之间的差距（0.838 - 0.821 = 0.017）。通用的N - BEATS模型尽可能少地使用先验知识，无需特征工程、归一化，也没有可被视为特定于时间序列的内部架构组件。因此，表1中的结果使我们得出结论：深度学习在广泛的时间序列预测任务中无需统计方法的支持，也无需手工制作的特征工程和领域知识就能表现得极为出色。除此之外，所提出的通用架构在众多不同数据集上表现良好，这表明广泛的模型类别，无论是通用的还是针对特定数据集手工制作的，都能表现出色，包括M4的获胜模型——一个针对M4数据的每个预测范围子集手动调整架构的模型。

### 3.2 数据集

M4数据集（M4团队，2018b；Makridakis等人，2018b）是Spyros Makridakis自1982年以来组织的一系列有影响力的预测竞赛中最新的一个（Makridakis等人，1982）。这个包含10万个序列的数据集规模庞大且多样，由商业、金融和经济预测中常见的数据组成，采样频率从每小时到每年不等。附录A.1中给出了一个包含汇总统计信息的表格，展示了时间序列特征的广泛变异性。

M3数据集（Makridakis和Hibon，2000）在组成上与M4相似，但总体规模较小（M3总共有3003个时间序列，而M4有10万个）。附录A.2中给出了一个包含汇总统计信息的表格。在过去20年里，这个数据集为设计更优的统计模型提供了有力支持，例如Theta方法及其变体（Assimakopoulos和Nikolopoulos，2000；Fiorucci等人，2016；Spiliotis等人，2019）。此外，最近一篇基于M3子集的论文（Makridakis等人，2018a）表明，机器学习模型不如经典统计模型。

TOURISM数据集（Athanasopoulos等人，2011）是作为Athanasopoulos和Hyndman（2011）举办的相应Kaggle竞赛的一部分发布的。数据包括由政府旅游组织（如澳大利亚旅游局、香港旅游发展局和新西兰旅游局）以及各学术界人士提供的月度、季度和年度序列，他们此前在研究中使用过这些数据。附录A.3中给出了一个包含汇总统计信息的表格。

### 3.3 训练方法

我们将每个数据集划分为训练集、验证集和测试集。测试集是每个数据集先前定义的标准测试集（M4团队，2018a；Makridakis和Hibon，2000；Athanasopoulos等人，2011）。每个数据集的验证集和训练集是通过在每个时间序列的最后一个预测范围边界处划分其完整训练集得到的。我们使用训练集和验证集来调整超参数。一旦确定了超参数，我们就在完整训练集上训练模型，并在测试集上报告结果。详细的超参数设置请参见附录D。

N - BEATS在Tensorflow中实现和训练（Abadi等人，2015）。我们在不同预测范围内共享网络参数，因此对于每个数据集的每个预测范围，我们训练一个模型。如果将每个时间序列视为一个单独的任务，这可以与多任务学习联系起来，并进一步与元学习联系起来（见第9节的讨论），在元学习中，神经网络针对多个任务进行正则化。我们希望不同预测范围和数据集的模型能够重用相同的架构。架构超参数（宽度、层数、堆叠数等）在不同预测范围和不同数据集（见附录D）中固定为相同的值。事实上，我们可以跨预测范围重用架构甚至超参数，这表明所提出的架构设计可以对不同性质的时间序列进行泛化。相同的架构已成功在包含4.8万个时间序列的M4月度子集和包含174个时间序列的M3其他子集上进行训练。这比例如Makridakis等人（2018b）的结果要强得多，他们不得不为不同的预测范围手工制作非常不同的架构。

为了更新一个预测范围的网络参数，我们对固定大小为1024的训练批次进行采样。我们从该预测范围中均匀随机地有放回地选择1024个时间序列id。对于每个选定的时间序列id，我们从历史范围$L_H$中随机选取一个预测点，其中$L_H$是一个经过交叉验证的超参数。我们观察到，对于时间序列数量较多的子集，$L_H$往往较小；对于时间序列数量较少的子集，$L_H$往往较大。例如，在M4的年度、月度、季度子集中，$L_H$等于1.5；在M4的中等到较小规模的周度、日度、小时度子集中，$L_H$等于10。给定一个采样的预测点，我们将长度为$2H$的历史窗口作为网络的输入$x$，其中$H$是目标预测窗口$y$的长度，我们设置一个等于$2H, 3H, \ldots, 7H$的预测范围。我们使用默认设置的Adam优化器，初始学习率为0.001。在优化集成成员以最小化sMAPE指标时，当分母中的梯度流导致数值不稳定时，我们停止训练。神经网络训练采用早停法，批次数量由验证集确定。在整个M4数据集上训练一个集成成员（基于GPU），根据神经网络设置和硬件的不同，耗时在30分钟到2小时之间。

### 3.4 可解释性结果

![20250418202110](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250418202110.png)

图2研究了所提出模型在通用配置和可解释配置下的输出。如3.3节所讨论的，为了使图1中呈现的通用架构具有可解释性，我们在第一个模块中约束$g_\theta$采用多项式形式，而第二个模块采用傅里叶基形式。此外，我们将通用配置下N - BEATS的输出（图1中描绘的30个残差块的通用模型分为两个堆叠）划分为控制组，并在图2中并排绘制通用（后缀“ - G”）和可解释（后缀“ - I”）堆叠输出。两个输出（通用模型和任意的、不可解释的模型）要么显示季节性，要么显示趋势，要么在两个堆叠的输出中都同时存在。第二个堆叠的输出幅度（峰峰值）通常较小。可解释模型的输出展现出不同的属性：趋势输出是单调的，平滑移动的季节性输出是规则的、周期性的且有反复波动。如果时间序列中存在显著的季节性，那么季节性输出的峰峰值幅度明显大于趋势的峰峰值幅度。类似地，如果真实信号中不存在明显趋势，趋势输出的幅度往往较小。因此，所提出的可解释架构将其预测分解为两个不同的组件。我们的结论是，通过在架构中编码可分解的归纳偏差，可以使深度学习模型的输出具有可解释性。表1证实了这一结果在性能上没有损失。
