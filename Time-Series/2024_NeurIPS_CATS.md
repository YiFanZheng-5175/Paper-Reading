# Are Self-Attentions Effective for Time Series Forecasting?

>领域：时间序列预测  
>发表在：NeurIPS 2024
>模型名字：***C***ross-***A***ttention-only Time Series ***T***ransformer  
>文章链接：[Are Self-Attentions Effective for Time Series Forecasting?](https://arxiv.org/abs/2405.16877)  
>代码仓库：[https://github.com/dongbeank/CATS](https://github.com/dongbeank/CATS)  
![20250316191321](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316191321.png)

## 一、研究背景与问题提出

### 1.1 研究现状

时间序列预测在机器学习领域中起着至关重要的作用，因为它的应用范围从金融预测到医疗诊断。为了提高预测的准确性，研究人员广泛探索并开发了各种模型。这些模型从传统的统计方法到现代深度学习技术都有。最值得注意的是，Transformer [19] 在时间序列预测中带来了范式转变，产生了许多高性能模型，如 Informer [29]、Autoformer [23]、Pyraformer [11]、FEDformer [31] 和 Crossformer [28]。这一系列工作为时间序列预测的高性能建立了新的基准。然而，曾等人[26]对基于 Transformer 的时间序列预测模型的有效性提出了质疑，特别是对于长期时间序列预测。具体来说，他们的实验表明，简单的线性模型可以胜过这些基于 Transformer 的方法，从而为更简单的架构框架的研究开辟了新的途径。事实上，后续的研究[12,6]进一步验证了这些线性模型可以通过纳入额外的特征来增强。

### 1.2 引出思考

尽管有这些发展，但Transformer架构中每个组件在时间序列预测中的有效性仍然是一个有争议的话题。Nie 等人[14]引入了一种仅编码器的 Transformer，它利用分块而不是逐点输入token，与线性模型相比表现出更好的性能。Zeng 等人[26]也强调了更简单的线性模型中潜在的缺点。网络存在一些问题，例如与 Transformer 相比，它们在变化点处无法捕捉时间动态[18]。***因此，虽然精简的架构可能是有益的，但必须严格评估 Transformer 的哪些元素对于时间序列建模是必要的，哪些不是。***

鉴于这些考虑，我们的研究将重点从 Transformer 的整体架构转移到一个更具体的问题上：自注意力对时间序列预测有效吗？虽然这个问题在[26]中也有提及，但他们的分析仅限于用线性层替代注意力层，在关注 Transformer 时，为潜在的模型设计留下了很大的空间。此外，时间信息丢失的问题（即自注意力的置换不变性和反序特性）主要是由自注意力的使用而不是 Transformer 架构本身引起的。因此，我们旨在解决自注意力的问题，并提出一种新的预测架构，以更高效的结构实现更高的性能。

## 二、问题剖析与解决策略

![20250316190154](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316190154.png)

在本文中，我们介绍了一种名为仅交叉注意力时间序列变换器（CATS）的新型预测架构，它通过消除所有自注意力并专注于交叉注意力的潜力来简化原始的变换器架构。具体来说，我们的模型将依赖于未来时间范围的参数建立为查询，并将过去的时间序列数据视为键值对。这使我们能够增强参数共享并提高长期预测性能。如图 1 所示，与现有模型相比，即使对于更长的输入序列和更少的参数，我们的模型也显示出最低的均方误差（即更好的预测性能）。此外，我们证明了这种简化的架构可以通过针对特定预测时间范围的单独注意力图，更清楚地了解未来预测是如何从过去的数据中得出的。最后，通过大量实验，我们表明，与之前的变换器模型相比，我们提出的模型在各种时间序列数据集上不仅实现了最先进的性能，而且需要更少的参数和更少的内存消耗。

### 2.1 Revisiting Self-Attention in Time Series Forecasting

随着对自注意力在时间信息保存方面有效性的担忧[26]，我们使用 PatchTST 进行了一项实验。我们考虑了 PatchTST 模型的三种变体：具有长度为 16 且步长为 8 的重叠补丁的原始 PatchTST（图 2a）；具有长度为 24 的非重叠补丁的修改后的 PatchTST（图 2b）；以及用线性嵌入层替换自注意力、使用长度为 24 的非重叠补丁的版本（图 2c）。这种设置使我们能够分离出自注意力对时间信息保存的影响，同时控制补丁重叠的影响。

![20250316190400](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316190400.png)

图 2 展示了这些模型变体在最终线性层中权重的绝对值。与原始的 PatchTST（图 2a）相比，两个非重叠版本（图 2b 和图 2c）显示出更生动的模式。具有线性嵌入的版本（图 2c）最清晰地捕捉到了时间信息，这表明自注意力机制本身对于捕捉时间信息可能不是必需的。

在表 1 中，我们总结了原始 PatchTST（图 2a）和无自注意力的 PatchTST（图 2c）的预测性能。无自注意力的 PatchTST 在所有预测范围内始终提高或保持性能。具体而言，具有自注意力的原始版本在较长的预测范围内表现较低。这一结果表明，自注意力对于有效的时间序列预测可能不仅不必要，甚至可能会阻碍在某些情况下的性能。因此，没有自注意力的更好性能挑战了关于自注意力机制在基于 Transformer 的时间序列预测任务模型中的重要性的传统观念。

![20250316190606](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316190606.png)

我们的研究结果为自我注意力在时间序列预测中的作用提供了新的见解。如图 2 和表 1 所示，用线性层替换自我注意力不仅能捕捉到清晰的时间模式，还能带来显著的性能提升，特别是对于较长的预测范围。这些结果突出了在处理时间信息方面的潜在改进领域，超越了对众所周知的计算复杂性问题的关注。

鉴于时间序列预测中自注意力相关的挑战，我们提出对用于此任务的 Transformer 架构进行根本性的重新思考。图 3 说明了现有架构与我们提出的方法之间的差异。传统的 Transformer 架构（图 3a）和仅编码器模型（图 3b）严重依赖自注意力机制，这可能导致时间信息丢失。相比之下，曾等人[26]提出了一种简化的线性模型 DLinear（图 3c），它去除了所有基于 Transformer 的组件。虽然这种方法减少了计算负荷，并可能避免一些时间信息丢失，但它可能难以捕捉复杂的时间依赖性。

![20250316190835](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316190835.png)

### 2.2 解决方法

#### 2.1.1 Cross-Attention with Future asQuery

与自注意力类似，交叉注意力机制采用三个元素：键、查询和值。交叉注意力的独特之处在于查询来自与键或值不同的来源。通常，查询组件旨在识别键中最相关的信息，并使用它从值中提取关键数据[1,27]。在时间序列预测领域，预测通常是针对特定的目标范围进行的，例如预测未来 10 步。因此，在这个预测概念中，我们认为每个未来的范围都应该被视为一个问题，即一个独立的查询。

为了实现这一点，我们将与时间跨度相关的参数建立为可学习的查询。如图 4 所示，我们首先为指定的预测时间跨度创建参数。对于这些虚拟化的参数中的每一个，我们分配固定数量的参数来表示相应的时间跨度，作为可学习的查询 4。例如，$q_{i}$ 是在 $L+i$ 的与时间跨度相关的查询。当应用补丁时，这些查询随后被独立处理；每个可学习的查询 $q \in \mathbb{R}^{P}$ 首先被输入到嵌入层，然后与作为键和值的嵌入输入时间序列补丁一起输入到多头注意力中。

基于这些新的查询参数，我们可以在解码器中使用仅交叉注意力的结构，从而在效率方面具有优势。在表 2 中，我们总结了最近的变压器模型和我们的时间复杂度。结果表明，我们的方法只需要 $O(L T / P^{2})$ 的时间复杂度，而大多数基于变压器的模型需要 $O(L^{2})$，除了 FEDformer 和、Pyraformer。然而，由于这两个模型具有编码器-解码器结构且参数数量相对庞大，它们分别需要比我们的模型多 10 倍和 4 倍的计算时间。

![20250316194710](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316194710.png)

#### 2.1.2 Parameter Sharing across Horizons

通过将未来预测步长作为查询向量$\mathbf{q}$来进行交叉注意力计算，其最大的优势之一在于，每个交叉注意力仅基于单个预测步长和输入时间序列的值进行计算。从数学角度来看，对于未来值$\hat{\mathbf{x}}_{L + i}$的预测，可以表示为一个仅依赖于过去样本$\mathbf{X} = [\mathbf{x}_1, \ldots, \mathbf{x}_L]$和$\mathbf{q}_i$的函数，且与所有$i \neq j$时的$\mathbf{q}_j$无关，或者说$i$和$j$不在同一块中。

这种独立的预测机制具有一个显著优势：更高程度的参数共享。如文献[14]所示，在时间序列预测中，通过输入或数据块之间的参数共享，可以显著减少所需的参数数量，从而提高计算效率。基于此，我们提出在所有可能的层（嵌入层、多头注意力层和投影层）之间进行参数共享，即对于每个与预测步长相关的查询$\mathbf{q}$。换句话说，多个预测步长$\mathbf{q}_1, \ldots, \mathbf{q}_T$或$\mathbf{q}_1, \ldots, \mathbf{q}_{N_T}$共享用于输入时间序列$\mathbf{x}_1, \ldots, \mathbf{x}_L$或数据块$\mathbf{p}_1, \ldots, \mathbf{p}_{N_L}$的同一嵌入层，然后再进入交叉注意力层。此外，为了最大化参数共享，我们还提出了跨维度共享，即对所有维度使用相同的查询参数。

对于多头注意力层和投影层，我们在不同预测步长上应用相同的算法。值得注意的是，与PatchTST（文献[14]）中的方法不同，我们还对每个预测的投影层进行参数共享。具体而言，PatchTST是一个仅编码器模型，它使用全连接层作为编码器输出$\mathbf{P} \in \mathbb{R}^{D \times N_L}$的投影层，这会产生$(D \times N_L) \times T$个参数。相比之下，我们的模型首先处理原始查询$\mathbf{q} = [\mathbf{q}_1, \ldots, \mathbf{q}_{N_T}] \in \mathbb{R}^{P \times N_T}$。然后，这些查询通过交叉注意力机制进行嵌入，得到$\mathbf{Q} = [\mathbf{q}_1, \ldots, \mathbf{q}_{N_T}] \in \mathbb{R}^{D \times N_T}$。最终的投影使用共享参数$\mathbf{W} \in \mathbb{R}^{P \times D}$，生成输出$\mathbf{W}\mathbf{Q} \in \mathbb{R}^{P \times N_T}$。因此，我们这个投影的参数数量变为$P \times D$，不会随$T$成比例增加。这种方法在训练和推理阶段都显著降低了时间复杂度。

在表3中，我们概述了跨不同预测步长进行参数共享的影响。与不进行参数共享的模型（随着预测步长的增加，其参数数量会迅速增加）不同，我们的模型共享包括投影层在内的所有层，参数数量几乎保持一致。

![20250316194948](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316194948.png)

此外，所有操作，包括嵌入和多头注意力，都是针对每个可学习的查询独立进行的。这意味着对特定预测步长的预测不依赖于其他预测步长。这种方法使我们能够为每个预测步长生成独特的注意力图，从而清晰地了解每个预测是如何得出的。请参考5.5节。

#### 2.1.3 Query-Adaptive Masking

跨预测步长的参数共享提高了我们所提出架构的效率，并简化了模型。然而，我们注意到，高度的参数共享可能会导致模型过度拟合键值对（即过去的时间序列数据），而不是查询（即预测步长）。具体来说，即使接收到不同的预测步长查询 $\mathbf{q}_i$ 和 $\mathbf{q}_j$ （即目标预测步长不同），模型也可能收敛到生成相似或相同的预测值 $\hat{\mathbf{x}}_{L + i}$ 和 $\hat{\mathbf{x}}_{L + j}$。

因此，为了确保模型专注于每个与预测步长相关的查询 $\mathbf{q}$，我们引入了一种对注意力输出进行掩码的新技术。如图4右下角的图所示，对于每个预测步长，我们以概率 $p$ 对多头注意力到层归一化（LayerNorm）的直接连接应用掩码。 该掩码阻止模型获取输入时间序列信息，从而使得只有查询能够影响未来值的预测。这种有选择的断开连接，而非在残差连接中应用丢弃（dropout），有助于各层更有效地专注于预测查询。我们注意到，这种方法与残差网络中的随机深度技术相关 。随机深度技术已在各种任务中被证明是有效的，比如视觉任务 [17, 25]。据我们所知，这是随机深度技术首次在Transformer模型中应用于时间序列预测。查询自适应掩码的详细分析可在附录中找到。

![20250316195320](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316195320.png)

### 2.2 模型结构

![20250316191321](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316191321.png)

## 三、实验验证与结果分析

### 3.1 消融实验

![20250316195806](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316195806.png)

### 3.2 通过交叉注意力解释周期性模式

![20250316195833](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316195833.png)

如4.2节所述，在我们提出的模型中，包括嵌入和多头注意力在内的所有操作，都是针对每个可学习的查询独立进行的。换句话说，对特定预测步长的预测并不依赖于其他预测步长。这种方法有助于我们更好地理解每个预测是如何得出的。因此，在本小节中，我们将可视化所提出的模型是如何理解周期性特性的。

为便于理解，我们考虑一个简单的时间序列预测任务，其数据由两个独立的信号组成，如下所示：
$$
\begin{align}
\mathbf{x}(t) &= \{x_{(t \text{ mod } \tau)}\}_{t = 1}^{\infty}, \quad x_{i} \sim \mathcal{N}(0, 1) \quad (i = 0, 1, \ldots, \tau - 1), \tag{1}\\
\mathbf{y}(t) &=
\begin{cases}
 +k & \text{if } t \equiv 0 \pmod{S}\\
 -k & \text{if } t \equiv \frac{1}{2}S \pmod{S}
\end{cases}
\tag{2}
\end{align}
$$

对于预测，我们使用输入序列长度$L = 48$，预测步长$T = 72$，其中信号$\mathbf{x}(t)$和$\mathbf{y}(t)$定义为$\tau = 24$，$S = 8$，$k = 5$。为了阐明具有2个注意力头的不同周期成分，数据块长度设置为4且无重叠。

在图6中，我们展示了经过训练的CATS模型的交叉注意力得分图（$12 \times 18$）。由于数据块长度和步幅都设置为4，每个数据块将恰好包含一个突变值。我们观察到，在图6a和图6b中，交叉注意力分别捕捉到了信号中的突变以及信号的周期性。图6a显示，在当前数据块之前偶数步的数据块包含相同方向的突变，从而具有更高的注意力得分，而奇数步的数据块得分较低。此外，如图6b所示，间隔为6步倍数的数据块清晰地展示了24步的相关性。这种周期性模式确保了注意力机制能够有效地捕捉$\mathbf{x}(t)$中的周期性，反映了模型利用这种周期性信息进行更准确预测的能力。在附录中，我们提供了详细的解释。

![20250316200045](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250316200045.png)
