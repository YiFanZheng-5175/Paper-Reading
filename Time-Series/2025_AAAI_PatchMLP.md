# Unlocking the Power of Patch: Patch-Based MLP for Long-Term Time Series Forecasting

>领域：时间序列预测  
>发表在：AAAI 2025  
>模型名字：PatchMLP  
>文章链接：[Unlocking the Power of Patch: Patch-Based MLP for Long-Term Time Series Forecasting](https://arxiv.org/abs/2405.13575)  
>代码仓库：[https://github.com/TangPeiwang/PatchMLP](https://github.com/TangPeiwang/PatchMLP)  
![20250422205154](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422205154.png)

## 一、研究背景与问题提出

### 1.1 研究现状

长期时间序列预测（LTSF）是统计学和机器学习领域的一个关键研究方向，旨在利用历史数据预测一个或多个变量在特定时期内的未来走势（奥列什金等人，2019；龚、唐和梁，2023；林，2023）。时间序列数据按时间顺序排列，揭示出潜在的动态模式，无论是周期性的还是非周期性的（克里尔，1986）。这种预测在多个领域都起着至关重要的作用，包括生物医学（刘等人，2018）、经济金融（帕顿，2013）、电力（周等人，2021）和交通（尹和尚，2016） 。

多元时间序列（MTS）由在同一时间点记录的多个变量组成，其中每个维度可以代表一个单变量时间序列，也可以被视为一个多通道信号。随着深度学习的发展，已经开发出许多模型来提高多元时间序列预测的性能（博罗夫伊赫、博特和奥斯特利，2017；白、科尔特和科尔顿，2018；刘等人，2020）。特别是，基于Transformer架构（瓦斯瓦尼等人，2017）的模型在捕捉长期依赖关系方面展现出巨大潜力（拉德福德等人，2018；德夫林等人，2018）。近年来，Transformer已成为时间序列预测中的主流架构（林等人，2021；刘等人，2022b；申等人，2023），最初应用于自然语言处理（NLP）领域（拉德福德等人，2019；布朗等人，2020），随后通过分块方法扩展到计算机视觉（CV）等其他领域（刘等人，2021b；费希滕霍费尔等人，2022），成为一种通用框架。

最初，这些模型采用通道混合方法（李等人，2023b），即将同一时间点记录的不同通道的向量投影到一个嵌入空间并混合信息（周等人，2021；唐和张，2023），并且许多模型采用时间序列分解的思想（吴等人，2021；周等人，2022；王等人，2023）。尽管如此，最近的研究表明，通道独立模型可能更有效（聂等人，2022；韩、叶和詹，2023）。通道独立意味着每个输入标记仅包含来自一个通道的信息，直观地说，这些模型将多元时间序列视为单独的单变量序列进行个体处理。这些研究表明，对于长期时间序列预测任务，强调通道独立性可能比通道混合策略更有效（龚、唐和梁，2023）。

此外，多层感知器（MLP）重新受到关注，对Transformer在长期时间序列预测任务中的有效性提出了挑战（曾等人，2023；埃坎巴拉姆等人，2023；陈等人，2023），其极其简单的架构性能超过了以往所有的Transformer模型。这就提出了一个引人深思的问题：Transformer对于长期时间序列预测任务是否有效？针对这种质疑，最近基于Transformer的模型采用了基于分块的表示方法，并在长期时间序列预测中取得了显著性能（聂等人，2022；陈等人，2024）。

### 1.2 问题提出

本研究提出了三个关键问题：

- 通道混合方法在多元时间序列预测中真的无效吗？
- 简单地分解原始时间序列真的能更好地预测趋势和季节成分吗？
- 基于分块的Transformer的出色性能是源于Transformer架构的内在优势，还是仅仅因为使用了分块作为输入表示？

## 二、问题剖析与解决策略

在本文中，我们详细讨论了上述三个问题，并介绍了PatchMLP，这是一种专为长期时间序列预测任务定制的新颖且简洁的基于分块的MLP模型 。PatchMLP完全基于全连接网络，并融入了分块的概念。具体来说，我们采用分块方法将原始序列嵌入到表示空间中，然后使用简单移动平均技术提取序列的平滑成分和噪声残差，以便分别进行处理。我们独立处理跨通道的随机噪声，并通过通道混合促进变量之间的语义信息交换。在对大量真实世界数据集进行评估后，PatchMLP模型展现出了领先水平（SOTA）的性能。确切地说，我们的贡献可以概括为三个方面：

- 我们分析了分块在时间序列预测中的有效性，并提出了多尺度分块嵌入（MPE）方法。与以往使用单个线性层的嵌入方法不同，MPE能够捕捉输入序列之间的多尺度关系。
- 我们引入了一种全新的完全基于MLP的模型，名为PatchMLP。通过使用移动平均，它对潜在向量进行分解，并采用不同的通道混合方法来实现变量间的语义信息交换。
- 我们在多个领域的广泛数据集上进行的实验表明，PatchMLP在多个预测基准上始终能达到领先水平的性能。此外，我们对基于分块的方法进行了广泛分析，旨在为时间序列预测的未来研究指明新方向。

### 2.1 分块在长期时间序列预测中的作用

![20250422204216](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422204216.png)
>图2：Patch Transformer在ETTh2数据集上的实验结果。（a）保持所有其他参数不变，仅改变输入长度，给出四种预测长度的均方误差（MSE）结果。（b）保持所有其他参数不变，仅改变分块大小，给出预测长度为720，五种不同输入长度下的MSE结果。（c）保持所有其他参数不变，仅改变分块大小，给出输入长度和预测长度均为720时，五个不同$d_{model}$值的MSE结果。

为什么分块在时间序列预测中有效？（李、朴和李，2023；钟等人，2023）我们研究了一种采用通道独立机制的基于分块的Transformer模型，并将解码器层设置为简单的单层MLP用于时间序列预测（聂等人，2022）。如图2b所示，我们可以清楚地看到，对于相同的输入长度，随着分块大小的增加，模型的整体均方误差（MSE）呈现出先减小后增大，或先减小后稳定的趋势。随着输入长度的增加，模型实现最优性能所需的分块大小也逐渐增大。在原始输入数据中，注意力机制的效果很差，甚至无法超越简单的单层MLP，这就引发了一个问题：注意力机制在某种程度上是否并非时间序列建模的最佳选择，因为它无法处理长期时间序列。

与自然语言处理领域的文本数据相比，原始时间序列数据由于高频采样包含许多冗余特征，并且在采样过程中在一定程度上容易受到噪声影响（唐和张，2022）。我们认为，对于原始数据中噪声过多的数据，原始Transformer的注意力机制并非最佳选择，因为它无法有效消除噪声。稀疏注意力表现更好，因为这种稀疏机制可以在一定程度上有效减轻噪声的影响（李等人，2019；吴等人，2020；刘等人，2021a），但它也可能削弱原始特征的影响。另一方面，分块对数据进行压缩，降低了输入数据的维度，减少了冗余特征。此外，分块提供了一定程度的平滑处理，这可以在一定程度上减少异常值的影响，有助于过滤掉波动和随机噪声，保留更稳定和有代表性的信息。在计算机视觉领域，分块在视觉Transformer（ViT）（多索维茨基等人，2020）、掩码自编码器（MAE）（贺等人，2022）中也表现良好。时间序列数据通常包含不同尺度的模式，分块提供了短期时间序列的建模，增强了序列中的局部信息，使模型能够更好地学习和捕捉局部特征。**因此，我们认为Transformer的有效性并非源于注意力机制的作用，而是因为分块的存在。**

那么，分块越大就越好吗？在图2b中，更大的分块似乎并没有取得更好的结果，这可能部分受到隐藏层大小（$d_{model}$）的影响。分块大小越大，就有更多的时间点被投影到固定大小的$d_{model}$中，这可能会导致过度压缩。如图2c所示，随着$d_{model}$的增加，模型的性能逐渐提高，表明更大的分块表现更好。在极端情况下，将整个输入序列视为一个分块并将其投影到一个向量的分块类似于iTransformer（刘等人，2023）。然而，更大的$d_{model}$意味着模型有更多的参数需要学习，这很容易导致欠拟合，从而降低模型性能。

### 2.2 解决方法

![20250422205154](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422205154.png)
>如图3所示，PatchMLP由四个网络组件组成：多尺度分块嵌入层、特征分解层、多层感知器（MLP）层、投影层。多尺度分块嵌入层将多元时间序列嵌入到潜在空间。特征分解层将潜在向量分解为平滑成分和含噪残差，然后通过MLP层分别处理。最后，潜在向量通过投影层映射回特征空间，以获得未来序列$\hat{\mathcal{X}}$ 。接下来，我们将分别介绍上述模块。

#### 多尺度分块嵌入

时间序列分析依赖于对序列中局部信息的准确识别以及模型的优化。分块的使用可以在短期内为模型提供时间序列的局部视图，从而增强序列中局部信息的表示，使模型能够更精确地学习和捕捉这些局部特征。然而，传统方法通常采用单一尺度的分块来嵌入原始时间序列，这使得模型倾向于学习单一尺度的时间关系，而忽略了时间序列数据的多尺度性质和复杂性。

在实际应用中，单一尺度的分块策略可能导致模型捕捉到不准确或不完整的局部特征。这是因为不同时间序列的数据特征往往存在差异，相同尺度的分块无法普遍适用于所有类型的时间序列。例如，对于包含多种周期性模式的时间序列，单一尺度的分块无法有效地识别和学习不同频率下存在的周期性特征。

为了更详细地捕捉局部信息并充分理解时间序列内的时间关系，我们采用多尺度分块。这包括较短的分块来捕捉局部高频模式，以及较长的分块来挖掘长期的季节性、趋势和周期性波动。通过引入多尺度分块，模型可以灵活地学习不同时间跨度上的代表性特征，从而提高预测准确性和模型泛化能力。

为了将多元时间序列$\mathcal{X}$分解为单变量序列$x$ ，我们开发了一组涵盖各种尺度的分块$\mathcal{P}$来处理$x$ 。对于特定尺度$p \in \mathcal{P}$ ，首先将$x$划分为不重叠的分块，分块长度对应于$p$ ，因此分块过程产生分块序列$x_p \in \mathbb{R}^{N \times p}$ ，其中$N$是分块数量。随后，我们使用单层线性层将分块$x_p$嵌入，得到潜在向量$x_e \in \mathbb{R}^{N \times d}$ ，$d$是嵌入维度，对应不同尺度分块的$d$可以不同。然后将这些潜在向量展开，得到最终的嵌入向量$X \in \mathbb{R}^{1 \times d_{model}}$ ，$d_{model}$是输入到模型的最终嵌入维度。这种多尺度分块策略使模型能够在不同层次上捕捉和学习时间序列的内在动态，从而在预测未来趋势和模式时提供更精确的见解。

#### 特征分解

时间序列通常包含复杂的时间模式，包括季节性波动、趋势和其他不规则影响因素。许多研究试图通过分解方法将时间序列分解为这些基本成分，以实现更好的理解和预测。尽管分解是一种强大的工具，但传统方法在处理原始序列时可能会遇到一些困难，特别是当序列中存在复杂和混合的模式时。这些方法往往难以准确分离出清晰的趋势和季节性成分，尤其是当数据包含大量噪声或非线性成分时。

针对这个问题，我们提出了一个不同的思路，不是直接分解原始序列，而是分解序列的嵌入向量。嵌入向量是通过将时间序列映射到高维空间形成的表示，这些表示通常捕捉到了原始数据的核心信息。通过分解嵌入向量，我们可以区分更平滑的成分和含噪残差，这有助于消除随机波动对分析和预测的干扰。在实践中，我们使用平均池化（$AvgPool$）操作来平滑时间序列，旨在减少数据中的随机波动和噪声。此外，为了在平滑时保持时间序列的长度不变，我们应用了填充操作。
$$
\begin{align}
X_s &= AvgPool(X) \\
X_r &= X - X_s
\end{align}
\tag{1}
$$
$X_s$和$X_r$分别表示提取的平滑成分和残差成分。通过对嵌入向量而不是原始序列进行操作，模型可以更精确地提取和识别时间序列的基本成分，从而更好地理解时间序列的内在结构，最终提高时间序列预测的准确性。

#### MLP层

在多元时间序列预测的背景下，MLP层交替在变量内（时域）和变量间（特征域）应用MLP，以提高预测性能。具体架构如图4所示，详细描述如下：

- **变量内MLP**：该组件专注于识别时间序列内的时间相关模式。具体来说，它利用由全连接层、非线性激活函数和丢弃层组成的网络架构。参数应用于时域，并在变量间共享。简单的线性模型易于理解和实现，并且已被证明能够学习复杂的时间模式并捕捉更复杂的时间依赖性。
- **变量间MLP**：作为变量内MLP的补充，变量间MLP旨在对多元时间序列变量之间的相互影响进行建模。该组件同样由全连接层、激活函数和丢弃层组成，在特征域应用全连接层，以实现变量内的参数共享。为了增强跨变量的交互性，我们利用点积机制，将MLP输出与输入之间的点积结果进行整合，这增强了模型的非线性表示能力。
- **残差连接**：在每个MLP之后应用残差连接（贺等人，2016） ，使模型能够更高效地学习更深层次的架构。这些连接不仅有助于缓解深度网络中的梯度消失问题，还为模型提供了一条捷径路径，确保关键的时间或特征信息不会被忽略。

虽然PatchMLP的极简架构可能不像最近提出的一些更复杂的Transformer模型那样引人注目，但实证结果表明，相对于领先水平（SOTA）模型，PatchMLP在训练效率和标准基准测试的准确性方面都表现出具有竞争力的性能。

#### 损失函数

我们的损失函数通过模型预测值$\hat{x}$与真实值$x$之间的均方误差（MSE）来计算：$\mathcal{L} = \frac{1}{M} \sum_{M}^{i = 1} \| \hat{x}_{L + 1 : L + T}^i - x_{L + 1 : L + T}^i \|_2^2$ ，并且损失从投影输出反向传播到整个模型。

## 三、实验验证与结果分析

### 消融实验

![20250422213917](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422213917.png)

如表2所示，我们研究了PatchMLP的每个模块对性能的影响。⑨采用传统的分解方法，即先分解时间序列，然后进行嵌入，再分别处理趋势和季节成分的预测。⑤去除了分解模块，直接将嵌入后的原始序列输入模型进行预测。在②和⑥中，移除了多尺度分块嵌入（MPE），并将嵌入方法改为单个线性层。③和⑦则去除了变量间MLP中的点积操作，代之以简单的残差连接，而④和⑧则完全移除了变量间的MLP，取消了它们之间的交互。我们仅改变相应模块，保持所有其他设置不变，输入长度仍设为96。

可以观察到，传统的分解方法表现不佳，这是因为原始时间序列中固有的时间关系过于复杂，无法通过简单的分解技术有效解构。相比之下，潜在向量分解能够妥善解决这一问题。此外，移除MPE后观察到的性能恶化表明，MPE在学习各种时间关系方面发挥着重要作用。另外值得注意的是，点积方法表现更优，仅仅是因为它增强了变量之间的交互性，而单纯的相加操作并不具备这一优势。相应地，缺乏变量交互性的模型性能会出现可预见的下降，这再次强调了合理利用变量间相互关系的至关重要性。

### 增加回溯窗口

原则上，更长的输入序列可能会提升模型性能，主要是因为它提供了更丰富的历史背景信息，有助于模型更有效地学习和识别时间序列中的长期模式。一般来说，一个强大的长期时间序列预测（LTSF）模型，若具备强大的时间关系提取能力，预计会随着输入长度的增加而产生更好的结果。然而，更长的输入要求模型具备更强的捕捉长期依赖关系的能力，因为在这方面的能力不足可能会迅速导致模型性能下降。

![20250422214259](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422214259.png)

如图5所示，随着输入长度的增加，所有模型的性能都逐渐提高，但当输入长度延长到相当长（768）时，一些模型开始出现性能下降，这也可能归因于随着输入扩展而增加的噪声。相比之下，我们的模型和DLinear随着输入长度的增加持续表现出稳定的性能提升，体现了线性模型在这种情况下的优越性。

### 超参数敏感性

我们评估了PatchMLP对以下参数的超敏感性：学习率（lr）、MLP中的块数（层数）N以及嵌入的隐藏维度D。结果如图6所示。我们观察到，PatchMLP的性能对这些超参数并不是特别敏感，表现为性能变化相对不明显。建议块数和隐藏维度大小都不应过大，这可能是因为这些超参数取值较大时，需要学习的参数总数会过多。然而，隐藏维度大小也不应过小，以避免将特征过度压缩到容量不足的潜在向量中。
