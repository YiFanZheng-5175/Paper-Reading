# Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift

>领域：时间序列预测  
>发表在：ICLR 2022  
>文章链接：[Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift](https://openreview.net/forum?id=cGDAkQo1C0p)  
![20250421231139](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250421231139.png)
![20250422152936](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422152936.png)

## 一、研究背景与问题提出

### 1.1 研究现状

时间序列预测方法主要分为三大类：（1）统计方法、（2）混合方法和（3）基于深度学习的方法。统计模型在理论上有很好的依据，并且有几个优点，包括可解释性。作为统计模型的一个例子，指数平滑预测（Holt, 2004; Winters, 1960）是预测未来值的成熟基准方法 。为了进一步提高性能，最近的研究提出了一种混合模型（Smy，2020），该模型将深度学习模块与统计模型相结合。在M4时间序列预测竞赛中，它的表现优于统计方法。基于深度学习的方法基本上遵循序列到序列的框架来对时间序列预测进行建模。最初，基于深度学习的模型使用循环神经网络（RNN）的各种变体。然而，为了克服有限感受野的局限性，一些研究采用了先进技术，如扩张和注意力模块。例如，SCINet（Liu等人，2021）和Informer（Zhou等人，2021）对序列到序列模型进行了改进以提高长期序列的性能。然而，与统计模型相比，大多数以前基于深度学习的模型难以解释。因此，受统计模型的启发，N - BEATS（Oreshkin等人，2020）通过促使模型明确学习趋势、季节性和残差成分，为时间序列预测设计了一个可解释层。该模型在M4竞赛数据集上表现优异。

虽然上述时间序列预测模型很多，但它们在处理非平稳时间序列时往往存在不足，因为非平稳时间序列的数据分布会随时间变化。领域自适应（Tzeng等人，2017; Ganin等人，2016; Wang等人，2018 ）和领域泛化（Wang等人，2021; Li等人，2018; Muandet等人，2013 ）是缓解分布偏移的常用方法。领域自适应算法试图缩小源域和目标域之间的分布差距。领域泛化算法仅依赖源域，并希望对目标域进行泛化。领域自适应和领域泛化都有一个共同目标，即弥合源域和目标域分布之间的差距。然而，在非平稳时间序列中定义一个领域并不容易，因为数据分布会随时间发生变化。最近，Du 等人（Du等人，2021）提出了自适应循环神经网络（Adaptive RNNs）来处理非平稳时间序列数据的分布偏移问题。它首先通过将训练数据划分为时间段来表征分布信息，然后匹配已发现时间段的分布以泛化模型。

如果我们从输入序列中去除非平稳信息，具体来说，就是实例的均值和标准差，数据分布的差异就会减小，从而提高模型性能。然而，对模型输入应用这种归一化处理可能会引发另一个问题，因为它可能会阻碍模型捕捉原始数据分布。去除非平稳信息对于预测未来值至关重要，而模型需要仅使用归一化后的输入来重建原始分布，这由于其内在局限性会降低模型的预测性能。因此，如果我们明确通过将归一化应用于模型输出层，将信息恢复到模型输出，而不是让模型自行重建原始分布，就能在保留输入归一化优势的同时避免上述问题。也就是说，要对模型输出进行归一化处理，使其使用归一化统计量。

## 二、问题剖析与解决策略

受此启发，我们提出了一种简单而有效的归一化 - 反归一化方法——可逆实例归一化（RevIN），它首先对输入序列进行归一化，然后对模型输出序列进行反归一化，以解决时间序列预测中对抗分布偏移的问题。RevIN 具有对称结构，通过在反归一化层中对输出进行缩放和平移，使输出恢复到原始分布信息，缩放和平移的量与归一化层中对输入数据的缩放和平移量相等。为了验证 RevIN 的有效性，我们使用几种最先进的时间序列预测方法作为基线进行了广泛的实证评估：Informer（Zhou等人，2021）、N - BEATS（Oreshkin等人，2020）和 SCINet（Liu等人，2021）。我们还深入分析了所提出方法的行为，并提供了关于可逆实例归一化假设的见解。

RevIN 是一个灵活的端到端可训练层，可以应用于任意选定的层，在一层中有效抑制非平稳信息（实例的均值和方差），并在另一个虚拟对称位置（如输入层和输出层）恢复该信息。尽管在时间序列领域将实例归一化 - 反归一化作为一个灵活适用的可训练层进行泛化和扩展是一项了不起的工作，但近期基于深度学习的时间序列预测方法，如 Informer（Zhou等人，2021）和 N - BEATS（Oreshkin等人，2020），在时间序列预测方面表现出色，却忽视了归一化的重要性，仅对模型输入使用简单的全局预处理，而没有进一步探索和预期其在端到端深度学习模型中的作用。尽管我们的方法很简单，但在现代基于深度学习的时间序列预测方法中，尚未有使用此类技术的案例（Zhou等人，2021；Liu等人，2021；Oreshkin等人，2020）。从这个意义上说，我们强调了在时间序列方法中应用实例归一化方法的重要性。我们通过结合可学习仿射变换方法（在近期基于深度学习的工作中已被广泛接受 （Ulyanov等人，2016）），精心设计了一个适用于深度学习的时间序列预测模块。

### 2.1 解决方法

#### RevIN

![20250422152936](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422152936.png)
>图2：所提方法概述。我们以单变量情况为例进行说明，其中$x^{(i)} \in \mathbb{R}^{1 \times T_x}$；实际输入数据$x^{(i)}$是多变量的（见3.1节）。在RevIN中，（a - 1）实例归一化和（a - 2）反归一化具有对称结构，用于从一层去除（a - 3）非平稳信息，并在另一层恢复该信息。此处，RevIN应用于输入层和输出层。（a - 3）非平稳信息包括来自输入数据的统计属性：均值$\mu$、方差$\sigma^2$以及可学习的仿射参数$\gamma$、$\beta$ 。反归一化层将（b - 1）原始数据分布转换为（b - 2）均值中心化分布，减少了不同实例之间的分布差异。利用$\hat{x}$，模型按照（b - 3）去除了非平稳信息的分布来预测未来值$\tilde{y}$。为了恢复该信息（b - 4），RevIN在输出层反转实例归一化操作。
给定一组输入$\mathcal{X} = \{x^{(i)}\}_{i = 1}^{N}$和相应的目标$\mathcal{Y} = \{y^{(i)}\}_{i = 1}^{N}$，我们考虑离散时间下的多变量时间序列预测任务，其中$N$表示序列的数量。令$K$、$T_x$和$T_y$分别表示变量数量、输入序列长度和模型预测长度。给定输入序列$x^{(i)} \in \mathbb{R}^{K \times T_x}$，我们旨在解决时间序列预测问题，即预测后续值$y^{(i)} \in \mathbb{R}^{K \times T_y}$。在RevIN中，输入序列长度$T_x$和预测长度$T_y$可以不同，因为观测值是在时间维度上进行归一化和反归一化的，如下所述。

我们提出的方法RevIN由对称结构的归一化和反归一化层组成，如图2所示。首先，我们使用输入数据$x^{(i)}$的实例特定均值和标准差对其进行归一化，这被广泛认为是实例归一化（Ulyanov等人，2016） 。对于输入数据的每个实例$x_{k.}^{(i)} \in \mathbb{R}^{T_x}$（图2(a - 3)），计算均值和标准差如下：

$$
\mathbb{E}_t[x_{kt}^{(i)}] = \frac{1}{T_x} \sum_{j = 1}^{T_x} x_{kj}^{(i)}
\quad \text{并且} \quad
\mathrm{Var}[x_{kt}^{(i)}] = \frac{1}{T_x} \sum_{j = 1}^{T_x} (x_{kj}^{(i)} - \mathbb{E}_t[x_{kt}^{(i)}])^2.
\tag{1}
$$

利用这些统计量，我们按如下方式对输入数据$x^{(i)}$进行归一化（图2(a - 1)）：

$$
\hat{x}_{kt}^{(i)} = \gamma_k \left( \frac{x_{kt}^{(i)} - \mathbb{E}_t[x_{kt}^{(i)}]}{\sqrt{\mathrm{Var}[x_{kt}^{(i)}] + \epsilon}} \right) + \beta_k,
\tag{2}
$$

其中$\gamma, \beta \in \mathbb{R}^{K}$是可学习的仿射参数向量。归一化后的序列可以具有更一致的均值和方差，减少了非平稳信息。因此，归一化层允许模型在接收均值和方差分布一致的输入时，准确预测序列内的局部动态。

然后，模型接收变换后的数据$\hat{x}^{(i)}$作为输入并预测其未来值。然而，输入数据与原始分布具有不同的统计量，仅观察归一化后的输入$\hat{x}^{(i)}$，很难捕捉到输入$x^{(i)}$的原始分布。因此，为了使模型更容易明确地恢复输入数据到模型输出中的非平稳属性，在对称位置（输出层）反转归一化步骤。反归一化步骤也可以将模型输出恢复为原始时间序列值（Ogasawara等人，2010） 。相应地，我们通过应用公式(2)中归一化的逆运算对模型输出$\tilde{y}^{(i)}$进行反归一化（图2(a - 3)）：

$$
\hat{y}_{kt}^{(i)} = \sqrt{\mathrm{Var}[x_{kt}^{(i)}] + \epsilon} \cdot \left( \frac{\tilde{y}_{kt}^{(i)} - \beta_k}{\gamma_k} \right) + \mathbb{E}_t[x_{kt}^{(i)}].
\tag{3}
$$

公式(2)中归一化步骤使用的相同统计量用于缩放和平移。现在，$\hat{y}^{(i)}$是模型的最终预测，而不是$\tilde{y}^{(i)}$。

RevIN只需添加到网络中几乎对称的位置，就可以有效地缓解时间序列数据中的分布差异，作为一种可应用于任意深度神经网络的通用可训练归一化层。实际上，所提出的方法是一个灵活的、端到端可训练的层，可以应用于任意选择的层，甚至是多个层。我们通过将其添加到附录A.4表7中模型的中间层来验证其作为灵活层的有效性。尽管如此，当应用于编码器 - 解码器结构的几乎对称层时，RevIN最为有效。在典型的时间序列预测模型中，编码器和解码器之间的边界往往不清晰。因此，我们将RevIN应用于模型的输入层和输出层，因为它们可以被解释为编码器 - 解码器结构，在给定输入数据的情况下生成后续值。

### 2.2 可逆实例归一化对分布偏移的影响

![20250422161206](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422161206.png)
>图3：RevIN对训练数据和测试数据之间分布差异的影响。从左到右列，我们比较了RevIN顺序过程每个步骤中一个变量的训练数据和测试数据分布：（a）原始输入$x$，（b）经RevIN归一化后的输入$\hat{x}$，（c）模型预测输出$\tilde{y}$，以及（d）经RevIN反归一化后的输出$\hat{y}$，即最终预测。该分析在ETT和ECL数据集上进行，使用SCINet（Liu等人，2021）作为基线。

本节验证了RevIN可以通过在输入层去除非平稳信息并在输出层恢复该信息，来缓解分布差异问题。我们分析了所提方法在每个步骤中训练数据和测试数据的分布，如图3所示。

在比较每个示例中训练数据和测试数据的分布时（图3(a - b)），我们可以观察到RevIN显著减少了它们之间的差异。具体而言，在原始输入中（图3(a)），训练数据和测试数据的分布几乎不重叠（尤其是ETTm₁），这是由分布偏移问题导致的。此外，每个数据分布都有多个峰值（尤其是ETTh₁和ECL的测试数据），这意味着数据中的序列在其分布上可能存在严重差异。然而，在所提方法中，归一化步骤将每个数据分布转换为均值中心化分布（图3(b)）。这一结果表明，原始的多峰分布（图3(a)）是由数据中不同序列之间的分布差异造成的。更进一步，所提方法使训练数据和测试数据的分布重叠。这验证了RevIN的归一化步骤可以缓解分布偏移问题，减少训练数据和测试数据之间的分布差异。

以归一化后的数据作为输入，模型在预测输出中能够保持训练数据和测试数据分布一致（图3(c)）。正如预期的那样，随后通过RevIN的反归一化步骤将其恢复到原始分布（图3(d)）。如果没有反归一化，模型需要仅使用遵循去除了非平稳信息的变换后分布的归一化输入，来重构遵循原始分布的值（图3(d)）。此外，我们假设，当仅在输入层和输出层应用RevIN时，模型中间层的分布差异也会减小，这将在4.2.3节中讨论。因此，RevIN过程可以被认为是先简化问题，然后将其恢复到原始状态，而不是直接解决存在分布偏移问题的具有挑战性的问题。

## 三、实验验证与结果分析

### 3.1 与现有归一化方法的比较

![20250422165629](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422165629.png)

我们将RevIN与经典及先进的归一化方法进行比较，包括最小 - 最大归一化、z - 分数归一化、层归一化（Ba等人，2016） 、批归一化（Ioffe & Szegedy，2015）、实例归一化（Ulyanov等人，2016）以及深度自适应输入归一化（DAIN）（Passalis等人，2019），结果见表3。在此，我们针对每个输入实例而非整个数据集计算最小 - 最大归一化和z - 分数归一化的统计量。此外，我们尝试将批归一化方法用于RevIN，称为可逆批归一化（RevBN）。层归一化不能以类似方式使用，因为在我们的实验设置中，当输入和预测长度不同时它是不可逆的。

结果表明，与其他归一化方法相比，RevIN表现出色，尤其是在ETTh₂和ETTm₁数据集上。此外，RevBN提高了批归一化的预测性能。具体而言，长序列预测中的误差大幅降低，例如960和1344。该结果支持反归一化步骤是RevIN的关键组成部分，是提高长序列预测的关键。然而，批归一化使用从整个训练数据获得的全局统计量对所有输入序列应用相同的归一化，它无法减少训练数据和测试数据分布之间的差异。因此，RevIN在实例层面显著改变数据，RevBN的性能也远超后者，这表明成功减少不同输入序列分布之间的差异可以有效提高性能。此外，RevIN不仅性能最佳，而且与基线方法相比还具有轻量级的优势。例如，当$K$为变量数量时，DAIN至少需要$3K^2$个额外参数，而RevIN仅需要$2K$个额外参数。

### 3.2 中间层分布偏移分析

![20250422170133](https://picgo-for-paper-reading.oss-cn-beijing.aliyuncs.com/img/20250422170133.png)

在图5中，我们分析训练数据和测试数据之间的特征散度，以验证RevIN是否也能在中间特征层减少分布偏移。我们使用Informer进行实验，它由两个编码器层和解码器层组成。因此，我们分析第一个（Layer - 1）和第二个（Layer - 2）编码器层的特征。根据先前工作（Pan等人，2018） ，我们使用对称KL散度计算平均特征散度（见附录A.10）。结果表明，RevIN显著减少了两层中训练数据和测试数据之间的特征散度，表明所提出的方法在仅添加到输入层和输出层时，成功缓解了中间层的分布偏移问题。此外，这强化了RevIN作为一种通用灵活层的特性。任意模型都可以通过将RevIN添加到输入层和输出层来采用它，而无需对架构进行任何修改。注意我们的方法仍然可以添加到任意选择的层中，显著提高模型性能，如附录A.4所示。
